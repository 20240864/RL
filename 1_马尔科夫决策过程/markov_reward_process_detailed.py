"""
é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹ (Markov Reward Process, MRP) è¯¦ç»†å®ç°

æœ¬æ–‡ä»¶æä¾›äº†MRPçš„å®Œæ•´å®ç°ï¼ŒåŒ…æ‹¬ï¼š
1. æ ¸å¿ƒæ•°å­¦ç†è®ºçš„ä¸¥æ ¼å®ç°
2. å¤šç§ä»·å€¼å‡½æ•°è®¡ç®—æ–¹æ³•
3. ä¸°å¯Œçš„å¯è§†åŒ–åŠŸèƒ½
4. è¯¦ç»†çš„æ•™å­¦ç¤ºä¾‹å’Œåˆ†æ

ä½œè€…ï¼šå¼ºåŒ–å­¦ä¹ æ•™å­¦ä»£ç 
ç‰ˆæœ¬ï¼š2.0
ç”¨é€”ï¼šæ·±å…¥ç†è§£é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹çš„æ•°å­¦åŸç†å’Œå®é™…åº”ç”¨
"""

import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
import seaborn as sns
from typing import List, Dict, Tuple, Optional, Union
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class MarkovRewardProcessDetailed:
    """
    é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹è¯¦ç»†å®ç°ç±»
    
    é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹æ˜¯ä¸€ä¸ªå››å…ƒç»„ (S, P, R, Î³)ï¼š
    - S: çŠ¶æ€ç©ºé—´ (State Space)
    - P: çŠ¶æ€è½¬ç§»æ¦‚ç‡çŸ©é˜µ (Transition Probability Matrix)
    - R: å¥–åŠ±å‡½æ•° (Reward Function)
    - Î³: æŠ˜æ‰£å› å­ (Discount Factor)
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. é©¬å°”å¯å¤«æ€§è´¨ï¼šP(S_{t+1} | S_t, S_{t-1}, ..., S_0) = P(S_{t+1} | S_t)
    2. ä»·å€¼å‡½æ•°ï¼šV(s) = E[G_t | S_t = s]ï¼Œå…¶ä¸­ G_t = Î£_{k=0}^âˆ Î³^k R_{t+k+1}
    3. è´å°”æ›¼æ–¹ç¨‹ï¼šV(s) = R(s) + Î³ * Î£_{s'} P(s'|s) * V(s')
    """
    
    def __init__(self, states: List[str], transition_matrix: np.ndarray, 
                 rewards: np.ndarray, gamma: float, state_descriptions: Optional[Dict[str, str]] = None):
        """
        åˆå§‹åŒ–é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹
        
        å‚æ•°:
            states: çŠ¶æ€åç§°åˆ—è¡¨
            transition_matrix: çŠ¶æ€è½¬ç§»æ¦‚ç‡çŸ©é˜µ P[i,j] = P(S_{t+1}=j | S_t=i)
            rewards: å¥–åŠ±å‡½æ•° R[i] = E[R_{t+1} | S_t=i]
            gamma: æŠ˜æ‰£å› å­ Î³ âˆˆ [0,1]
            state_descriptions: çŠ¶æ€æè¿°å­—å…¸ï¼ˆå¯é€‰ï¼‰
        """
        self.states = states
        self.n_states = len(states)
        self.state_to_index = {state: i for i, state in enumerate(states)}
        self.transition_matrix = np.array(transition_matrix)
        self.rewards = np.array(rewards)
        self.gamma = gamma
        self.state_descriptions = state_descriptions or {}
        
        # éªŒè¯è¾“å…¥å‚æ•°
        self._validate_parameters()
        
        # å­˜å‚¨è®¡ç®—å†å²
        self.convergence_history = []
        self.analytical_solution = None
        self.iterative_solution = None
        
        print(f"âœ… é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹åˆå§‹åŒ–å®Œæˆ")
        print(f"   çŠ¶æ€æ•°é‡: {self.n_states}")
        print(f"   æŠ˜æ‰£å› å­: {self.gamma}")
        print(f"   çŠ¶æ€åˆ—è¡¨: {self.states}")
    
    def _validate_parameters(self):
        """éªŒè¯è¾“å…¥å‚æ•°çš„æœ‰æ•ˆæ€§"""
        # æ£€æŸ¥è½¬ç§»æ¦‚ç‡çŸ©é˜µ
        if self.transition_matrix.shape != (self.n_states, self.n_states):
            raise ValueError(f"è½¬ç§»æ¦‚ç‡çŸ©é˜µå½¢çŠ¶é”™è¯¯: æœŸæœ› {(self.n_states, self.n_states)}, å®é™… {self.transition_matrix.shape}")
        
        # æ£€æŸ¥æ¦‚ç‡çŸ©é˜µæ¯è¡Œå’Œä¸º1
        row_sums = np.sum(self.transition_matrix, axis=1)
        if not np.allclose(row_sums, 1.0, rtol=1e-10):
            raise ValueError(f"è½¬ç§»æ¦‚ç‡çŸ©é˜µæ¯è¡Œå’Œå¿…é¡»ä¸º1, å®é™…: {row_sums}")
        
        # æ£€æŸ¥æ¦‚ç‡éè´Ÿ
        if np.any(self.transition_matrix < 0):
            raise ValueError("è½¬ç§»æ¦‚ç‡ä¸èƒ½ä¸ºè´Ÿæ•°")
        
        # æ£€æŸ¥å¥–åŠ±å‡½æ•°
        if len(self.rewards) != self.n_states:
            raise ValueError(f"å¥–åŠ±å‡½æ•°é•¿åº¦é”™è¯¯: æœŸæœ› {self.n_states}, å®é™… {len(self.rewards)}")
        
        # æ£€æŸ¥æŠ˜æ‰£å› å­
        if not 0 <= self.gamma <= 1:
            raise ValueError(f"æŠ˜æ‰£å› å­å¿…é¡»åœ¨[0,1]èŒƒå›´å†…, å®é™…: {self.gamma}")
    
    def compute_value_function_analytical(self) -> np.ndarray:
        """
        ä½¿ç”¨è§£æè§£è®¡ç®—ä»·å€¼å‡½æ•°
        
        è´å°”æ›¼æ–¹ç¨‹çš„çŸ©é˜µå½¢å¼ï¼šV = R + Î³PV
        é‡æ–°æ•´ç†å¾—åˆ°ï¼šV = (I - Î³P)^(-1) * R
        
        è¿”å›:
            ä»·å€¼å‡½æ•°å‘é‡ V
        """
        print("\nğŸ§® è®¡ç®—ä»·å€¼å‡½æ•° - è§£æè§£æ–¹æ³•")
        print("=" * 50)
        
        try:
            # è®¡ç®— (I - Î³P)
            I = np.eye(self.n_states)
            matrix_to_invert = I - self.gamma * self.transition_matrix
            
            print(f"è®¡ç®—çŸ©é˜µ (I - Î³P):")
            print(f"I (å•ä½çŸ©é˜µ):\n{I}")
            print(f"Î³P (æŠ˜æ‰£è½¬ç§»çŸ©é˜µ):\n{self.gamma * self.transition_matrix}")
            print(f"(I - Î³P):\n{matrix_to_invert}")
            
            # æ£€æŸ¥çŸ©é˜µæ˜¯å¦å¯é€†
            det = np.linalg.det(matrix_to_invert)
            print(f"çŸ©é˜µè¡Œåˆ—å¼: {det:.6f}")
            
            if abs(det) < 1e-10:
                raise np.linalg.LinAlgError("çŸ©é˜µæ¥è¿‘å¥‡å¼‚ï¼Œæ— æ³•æ±‚é€†")
            
            # è®¡ç®—é€†çŸ©é˜µ
            inverse_matrix = np.linalg.inv(matrix_to_invert)
            print(f"é€†çŸ©é˜µ (I - Î³P)^(-1):\n{inverse_matrix}")
            
            # è®¡ç®—ä»·å€¼å‡½æ•°
            self.analytical_solution = inverse_matrix @ self.rewards
            
            print(f"\nâœ… è§£æè§£è®¡ç®—å®Œæˆ")
            print("ä»·å€¼å‡½æ•°ç»“æœ:")
            for i, (state, value) in enumerate(zip(self.states, self.analytical_solution)):
                print(f"  V({state}) = {value:.6f}")
            
            return self.analytical_solution
            
        except np.linalg.LinAlgError as e:
            print(f"âŒ è§£æè§£è®¡ç®—å¤±è´¥: {e}")
            print("å¯èƒ½åŸå› : Î³ = 1 ä¸”å­˜åœ¨å¸æ”¶çŠ¶æ€ï¼Œæˆ–çŸ©é˜µå¥‡å¼‚")
            return None
    
    def compute_value_function_iterative(self, max_iterations: int = 1000, 
                                       tolerance: float = 1e-8, verbose: bool = True) -> np.ndarray:
        """
        ä½¿ç”¨ä»·å€¼è¿­ä»£ç®—æ³•è®¡ç®—ä»·å€¼å‡½æ•°
        
        è¿­ä»£å…¬å¼ï¼šV_{k+1}(s) = R(s) + Î³ * Î£_{s'} P(s'|s) * V_k(s')
        
        å‚æ•°:
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            tolerance: æ”¶æ•›å®¹å¿åº¦
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        è¿”å›:
            ä»·å€¼å‡½æ•°å‘é‡ V
        """
        if verbose:
            print("\nğŸ”„ è®¡ç®—ä»·å€¼å‡½æ•° - ä»·å€¼è¿­ä»£æ–¹æ³•")
            print("=" * 50)
            print(f"æœ€å¤§è¿­ä»£æ¬¡æ•°: {max_iterations}")
            print(f"æ”¶æ•›å®¹å¿åº¦: {tolerance}")
        
        # åˆå§‹åŒ–ä»·å€¼å‡½æ•°
        V = np.zeros(self.n_states)
        self.convergence_history = []
        
        for iteration in range(max_iterations):
            # ä¿å­˜ä¸Šä¸€æ¬¡çš„ä»·å€¼å‡½æ•°
            V_old = V.copy()
            
            # ä»·å€¼è¿­ä»£æ›´æ–°
            V_new = self.rewards + self.gamma * (self.transition_matrix @ V)
            
            # è®¡ç®—å˜åŒ–é‡
            delta = np.max(np.abs(V_new - V))
            self.convergence_history.append(delta)
            
            if verbose and (iteration < 10 or iteration % 50 == 0):
                print(f"è¿­ä»£ {iteration:3d}: æœ€å¤§å˜åŒ–é‡ = {delta:.8f}")
                if iteration < 5:  # æ˜¾ç¤ºå‰å‡ æ¬¡è¿­ä»£çš„è¯¦ç»†ä¿¡æ¯
                    print(f"  V = {V_new}")
            
            # æ£€æŸ¥æ”¶æ•›
            if delta < tolerance:
                if verbose:
                    print(f"âœ… åœ¨ç¬¬ {iteration} æ¬¡è¿­ä»£åæ”¶æ•›")
                    print(f"æœ€ç»ˆæ”¶æ•›ç²¾åº¦: {delta:.10f}")
                break
            
            V = V_new
        else:
            if verbose:
                print(f"âš ï¸  è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}ï¼Œæœªå®Œå…¨æ”¶æ•›")
                print(f"æœ€ç»ˆå˜åŒ–é‡: {delta:.8f}")
        
        self.iterative_solution = V
        
        if verbose:
            print("\nä»·å€¼å‡½æ•°ç»“æœ:")
            for i, (state, value) in enumerate(zip(self.states, V)):
                print(f"  V({state}) = {value:.6f}")
        
        return V
    
    def simulate_episode(self, start_state: str, max_steps: int = 100, 
                        random_seed: Optional[int] = None) -> Tuple[List[str], List[float]]:
        """
        æ¨¡æ‹Ÿä¸€ä¸ªå›åˆçš„çŠ¶æ€è½¬ç§»å’Œå¥–åŠ±åºåˆ—
        
        å‚æ•°:
            start_state: èµ·å§‹çŠ¶æ€
            max_steps: æœ€å¤§æ­¥æ•°
            random_seed: éšæœºç§å­
            
        è¿”å›:
            (çŠ¶æ€åºåˆ—, å¥–åŠ±åºåˆ—)
        """
        if random_seed is not None:
            np.random.seed(random_seed)
        
        if start_state not in self.state_to_index:
            raise ValueError(f"èµ·å§‹çŠ¶æ€ '{start_state}' ä¸åœ¨çŠ¶æ€ç©ºé—´ä¸­")
        
        states_sequence = [start_state]
        rewards_sequence = []
        current_state_idx = self.state_to_index[start_state]
        
        for step in range(max_steps):
            # è·å–å½“å‰çŠ¶æ€çš„å¥–åŠ±
            reward = self.rewards[current_state_idx]
            rewards_sequence.append(reward)
            
            # æ ¹æ®è½¬ç§»æ¦‚ç‡é€‰æ‹©ä¸‹ä¸€ä¸ªçŠ¶æ€
            transition_probs = self.transition_matrix[current_state_idx]
            next_state_idx = np.random.choice(self.n_states, p=transition_probs)
            next_state = self.states[next_state_idx]
            
            states_sequence.append(next_state)
            current_state_idx = next_state_idx
        
        return states_sequence, rewards_sequence
    
    def compute_discounted_return(self, rewards_sequence: List[float]) -> float:
        """
        è®¡ç®—æŠ˜æ‰£å›æŠ¥ G_t = Î£_{k=0}^âˆ Î³^k * R_{t+k+1}
        
        å‚æ•°:
            rewards_sequence: å¥–åŠ±åºåˆ—
            
        è¿”å›:
            æŠ˜æ‰£å›æŠ¥
        """
        return sum(reward * (self.gamma ** t) for t, reward in enumerate(rewards_sequence))
    
    def analyze_stationary_distribution(self) -> np.ndarray:
        """
        åˆ†æé©¬å°”å¯å¤«é“¾çš„å¹³ç¨³åˆ†å¸ƒ
        
        å¹³ç¨³åˆ†å¸ƒ Ï€ æ»¡è¶³ï¼šÏ€ = Ï€Pï¼Œå³ Ï€(I - P) = 0
        
        è¿”å›:
            å¹³ç¨³åˆ†å¸ƒå‘é‡
        """
        print("\nğŸ“Š åˆ†æé©¬å°”å¯å¤«é“¾çš„å¹³ç¨³åˆ†å¸ƒ")
        print("=" * 50)
        
        try:
            # è®¡ç®—è½¬ç§»çŸ©é˜µçš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
            eigenvalues, eigenvectors = np.linalg.eig(self.transition_matrix.T)
            
            # æ‰¾åˆ°ç‰¹å¾å€¼ä¸º1çš„ç‰¹å¾å‘é‡ï¼ˆå¹³ç¨³åˆ†å¸ƒï¼‰
            stationary_idx = np.argmin(np.abs(eigenvalues - 1.0))
            stationary_vector = np.real(eigenvectors[:, stationary_idx])
            
            # å½’ä¸€åŒ–ä½¿å…¶æˆä¸ºæ¦‚ç‡åˆ†å¸ƒ
            stationary_distribution = stationary_vector / np.sum(stationary_vector)
            
            # ç¡®ä¿éè´Ÿ
            if np.any(stationary_distribution < 0):
                stationary_distribution = np.abs(stationary_distribution)
                stationary_distribution = stationary_distribution / np.sum(stationary_distribution)
            
            print("å¹³ç¨³åˆ†å¸ƒç»“æœ:")
            for state, prob in zip(self.states, stationary_distribution):
                print(f"  Ï€({state}) = {prob:.6f}")
            
            # éªŒè¯å¹³ç¨³åˆ†å¸ƒ
            verification = stationary_distribution @ self.transition_matrix
            error = np.max(np.abs(verification - stationary_distribution))
            print(f"\néªŒè¯è¯¯å·®: {error:.10f}")
            
            return stationary_distribution
            
        except Exception as e:
            print(f"âŒ å¹³ç¨³åˆ†å¸ƒè®¡ç®—å¤±è´¥: {e}")
            return None
    
    def print_detailed_summary(self):
        """æ‰“å°è¯¦ç»†çš„MRPä¿¡æ¯æ‘˜è¦"""
        print("\n" + "=" * 80)
        print("é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹ (MRP) è¯¦ç»†ä¿¡æ¯æ‘˜è¦")
        print("=" * 80)
        
        print(f"\nğŸ“‹ åŸºæœ¬ä¿¡æ¯:")
        print(f"   çŠ¶æ€ç©ºé—´å¤§å°: {self.n_states}")
        print(f"   çŠ¶æ€åˆ—è¡¨: {self.states}")
        print(f"   æŠ˜æ‰£å› å­ Î³: {self.gamma}")
        
        if self.state_descriptions:
            print(f"\nğŸ“ çŠ¶æ€æè¿°:")
            for state, desc in self.state_descriptions.items():
                print(f"   {state}: {desc}")
        
        print(f"\nğŸ¯ å¥–åŠ±å‡½æ•°:")
        for i, (state, reward) in enumerate(zip(self.states, self.rewards)):
            print(f"   R({state}) = {reward:8.3f}")
        
        print(f"\nğŸ”„ çŠ¶æ€è½¬ç§»æ¦‚ç‡çŸ©é˜µ:")
        print("     " + "".join(f"{state:>8}" for state in self.states))
        for i, from_state in enumerate(self.states):
            row_str = f"{from_state:>4} "
            for j in range(self.n_states):
                row_str += f"{self.transition_matrix[i, j]:8.3f}"
            print(row_str)
        
        # æ˜¾ç¤ºä»·å€¼å‡½æ•°ç»“æœ
        if self.analytical_solution is not None or self.iterative_solution is not None:
            print(f"\nğŸ’° ä»·å€¼å‡½æ•°ç»“æœ:")
            if self.analytical_solution is not None:
                print("   è§£æè§£:")
                for state, value in zip(self.states, self.analytical_solution):
                    print(f"     V({state}) = {value:10.6f}")
            
            if self.iterative_solution is not None:
                print("   è¿­ä»£è§£:")
                for state, value in zip(self.states, self.iterative_solution):
                    print(f"     V({state}) = {value:10.6f}")
                
                if self.analytical_solution is not None:
                    print("   å·®å¼‚åˆ†æ:")
                    for state, v_ana, v_iter in zip(self.states, self.analytical_solution, self.iterative_solution):
                        diff = abs(v_ana - v_iter)
                        print(f"     |V_ana({state}) - V_iter({state})| = {diff:.8f}")


class MRPVisualizer:
    """é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹å¯è§†åŒ–ç±»"""
    
    def __init__(self, mrp: MarkovRewardProcessDetailed):
        """
        åˆå§‹åŒ–å¯è§†åŒ–å™¨
        
        å‚æ•°:
            mrp: é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹å®ä¾‹
        """
        self.mrp = mrp
        plt.style.use('default')  # ä½¿ç”¨é»˜è®¤æ ·å¼
        # é‡æ–°è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œç¡®ä¿åœ¨æ ·å¼è®¾ç½®åç”Ÿæ•ˆ
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
    def plot_state_transition_graph(self, figsize: Tuple[int, int] = (12, 8), 
                                  min_edge_weight: float = 0.01):
        """
        ç»˜åˆ¶çŠ¶æ€è½¬ç§»å›¾
        
        å‚æ•°:
            figsize: å›¾å½¢å¤§å°
            min_edge_weight: æœ€å°è¾¹æƒé‡é˜ˆå€¼
        """
        fig, ax = plt.subplots(figsize=figsize)
        
        # åˆ›å»ºæœ‰å‘å›¾
        G = nx.DiGraph()
        
        # æ·»åŠ èŠ‚ç‚¹
        for i, state in enumerate(self.mrp.states):
            reward = self.mrp.rewards[i]
            G.add_node(state, reward=reward)
        
        # æ·»åŠ è¾¹ï¼ˆåªæ˜¾ç¤ºæ¦‚ç‡å¤§äºé˜ˆå€¼çš„è½¬ç§»ï¼‰
        for i, from_state in enumerate(self.mrp.states):
            for j, to_state in enumerate(self.mrp.states):
                prob = self.mrp.transition_matrix[i, j]
                if prob > min_edge_weight:
                    G.add_edge(from_state, to_state, weight=prob)
        
        # è®¾ç½®å¸ƒå±€
        pos = nx.spring_layout(G, k=3, iterations=50)
        
        # ç»˜åˆ¶èŠ‚ç‚¹
        node_colors = []
        node_sizes = []
        for state in G.nodes():
            reward = G.nodes[state]['reward']
            if reward > 0:
                node_colors.append('lightgreen')
            elif reward < 0:
                node_colors.append('lightcoral')
            else:
                node_colors.append('lightblue')
            node_sizes.append(max(500, abs(reward) * 50 + 300))
        
        nx.draw_networkx_nodes(G, pos, node_color=node_colors, 
                              node_size=node_sizes, alpha=0.8)
        
        # ç»˜åˆ¶è¾¹
        edges = G.edges()
        weights = [G[u][v]['weight'] for u, v in edges]
        nx.draw_networkx_edges(G, pos, edgelist=edges, width=[w*3 for w in weights],
                              alpha=0.6, edge_color='gray', arrows=True, 
                              arrowsize=20, arrowstyle='->')
        
        # æ·»åŠ èŠ‚ç‚¹æ ‡ç­¾
        nx.draw_networkx_labels(G, pos, font_size=12, font_weight='bold')
        
        # æ·»åŠ è¾¹æ ‡ç­¾ï¼ˆè½¬ç§»æ¦‚ç‡ï¼‰
        edge_labels = {(u, v): f'{G[u][v]["weight"]:.2f}' for u, v in edges}
        nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=10)
        
        # æ·»åŠ å¥–åŠ±ä¿¡æ¯
        reward_labels = {}
        for state in G.nodes():
            reward = G.nodes[state]['reward']
            reward_labels[state] = f'R={reward:.1f}'
        
        # è°ƒæ•´æ ‡ç­¾ä½ç½®
        pos_rewards = {node: (x, y-0.15) for node, (x, y) in pos.items()}
        for node, (x, y) in pos_rewards.items():
            ax.text(x, y, reward_labels[node], ha='center', va='center',
                   bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),
                   fontsize=10)
        
        ax.set_title('é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹ - çŠ¶æ€è½¬ç§»å›¾\n'
                    f'(Î³={self.mrp.gamma}, åªæ˜¾ç¤ºæ¦‚ç‡>{min_edge_weight}çš„è½¬ç§»)', 
                    fontsize=14, fontweight='bold')
        ax.axis('off')
        
        # æ·»åŠ å›¾ä¾‹
        legend_elements = [
            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgreen', 
                      markersize=10, label='æ­£å¥–åŠ±çŠ¶æ€'),
            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightcoral', 
                      markersize=10, label='è´Ÿå¥–åŠ±çŠ¶æ€'),
            plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightblue', 
                      markersize=10, label='é›¶å¥–åŠ±çŠ¶æ€')
        ]
        ax.legend(handles=legend_elements, loc='upper right')
        
        plt.tight_layout()
        plt.show()
    
    def plot_transition_matrix_heatmap(self, figsize: Tuple[int, int] = (10, 8)):
        """ç»˜åˆ¶è½¬ç§»æ¦‚ç‡çŸ©é˜µçƒ­åŠ›å›¾"""
        fig, ax = plt.subplots(figsize=figsize)
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        im = ax.imshow(self.mrp.transition_matrix, cmap='Blues', aspect='auto')
        
        # è®¾ç½®åæ ‡è½´æ ‡ç­¾
        ax.set_xticks(range(self.mrp.n_states))
        ax.set_yticks(range(self.mrp.n_states))
        ax.set_xticklabels(self.mrp.states)
        ax.set_yticklabels(self.mrp.states)
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for i in range(self.mrp.n_states):
            for j in range(self.mrp.n_states):
                prob = self.mrp.transition_matrix[i, j]
                color = 'white' if prob > 0.5 else 'black'
                ax.text(j, i, f'{prob:.3f}', ha='center', va='center', 
                       color=color, fontweight='bold')
        
        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(im, ax=ax)
        cbar.set_label('è½¬ç§»æ¦‚ç‡', rotation=270, labelpad=20)
        
        ax.set_title('çŠ¶æ€è½¬ç§»æ¦‚ç‡çŸ©é˜µçƒ­åŠ›å›¾\nP[i,j] = P(S_{t+1}=j | S_t=i)', 
                    fontsize=14, fontweight='bold')
        ax.set_xlabel('ç›®æ ‡çŠ¶æ€ (S_{t+1})')
        ax.set_ylabel('å½“å‰çŠ¶æ€ (S_t)')
        
        plt.tight_layout()
        plt.show()
    
    def plot_value_function_comparison(self, figsize: Tuple[int, int] = (12, 6)):
        """ç»˜åˆ¶ä»·å€¼å‡½æ•°æ¯”è¾ƒå›¾"""
        if self.mrp.analytical_solution is None and self.mrp.iterative_solution is None:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„ä»·å€¼å‡½æ•°ç»“æœè¿›è¡Œæ¯”è¾ƒ")
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)
        
        # å·¦å›¾ï¼šä»·å€¼å‡½æ•°æŸ±çŠ¶å›¾
        x = np.arange(len(self.mrp.states))
        width = 0.35
        
        if self.mrp.analytical_solution is not None:
            bars1 = ax1.bar(x - width/2, self.mrp.analytical_solution, width, 
                           label='è§£æè§£', alpha=0.8, color='skyblue')
        
        if self.mrp.iterative_solution is not None:
            bars2 = ax1.bar(x + width/2, self.mrp.iterative_solution, width,
                           label='è¿­ä»£è§£', alpha=0.8, color='lightcoral')
        
        ax1.set_xlabel('çŠ¶æ€')
        ax1.set_ylabel('ä»·å€¼å‡½æ•° V(s)')
        ax1.set_title('ä»·å€¼å‡½æ•°æ¯”è¾ƒ')
        ax1.set_xticks(x)
        ax1.set_xticklabels(self.mrp.states)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        if self.mrp.analytical_solution is not None:
            for i, v in enumerate(self.mrp.analytical_solution):
                ax1.text(i - width/2, v + 0.5, f'{v:.2f}', ha='center', va='bottom')
        
        if self.mrp.iterative_solution is not None:
            for i, v in enumerate(self.mrp.iterative_solution):
                ax1.text(i + width/2, v + 0.5, f'{v:.2f}', ha='center', va='bottom')
        
        # å³å›¾ï¼šæ”¶æ•›å†å²
        if self.mrp.convergence_history:
            ax2.semilogy(self.mrp.convergence_history, 'b-', linewidth=2)
            ax2.set_xlabel('è¿­ä»£æ¬¡æ•°')
            ax2.set_ylabel('æœ€å¤§å˜åŒ–é‡ (å¯¹æ•°å°ºåº¦)')
            ax2.set_title('ä»·å€¼è¿­ä»£æ”¶æ•›å†å²')
            ax2.grid(True, alpha=0.3)
            
            # æ ‡æ³¨æ”¶æ•›ç‚¹
            final_error = self.mrp.convergence_history[-1]
            ax2.axhline(y=final_error, color='red', linestyle='--', alpha=0.7)
            ax2.text(len(self.mrp.convergence_history)*0.7, final_error*2, 
                    f'æœ€ç»ˆè¯¯å·®: {final_error:.2e}', fontsize=10)
        else:
            ax2.text(0.5, 0.5, 'æ— æ”¶æ•›å†å²æ•°æ®', ha='center', va='center', 
                    transform=ax2.transAxes, fontsize=12)
            ax2.set_title('ä»·å€¼è¿­ä»£æ”¶æ•›å†å²')
        
        plt.tight_layout()
        plt.show()
    
    def plot_reward_function(self, figsize: Tuple[int, int] = (10, 6)):
        """ç»˜åˆ¶å¥–åŠ±å‡½æ•°å›¾"""
        fig, ax = plt.subplots(figsize=figsize)
        
        # åˆ›å»ºé¢œè‰²æ˜ å°„
        colors = ['red' if r < 0 else 'green' if r > 0 else 'gray' for r in self.mrp.rewards]
        
        bars = ax.bar(self.mrp.states, self.mrp.rewards, color=colors, alpha=0.7)
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for bar, reward in zip(bars, self.mrp.rewards):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + (0.5 if height >= 0 else -0.5),
                   f'{reward:.1f}', ha='center', va='bottom' if height >= 0 else 'top')
        
        ax.set_xlabel('çŠ¶æ€')
        ax.set_ylabel('å¥–åŠ±å€¼ R(s)')
        ax.set_title('é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹ - å¥–åŠ±å‡½æ•°')
        ax.grid(True, alpha=0.3)
        ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)
        
        plt.tight_layout()
        plt.show()
    
    def plot_episode_simulation(self, start_state: str, n_episodes: int = 20, 
                              max_steps: int = 15, figsize: Tuple[int, int] = (15, 10)):
        """
        ç»˜åˆ¶å›åˆæ¨¡æ‹Ÿåˆ†æå›¾
        
        å‚æ•°:
            start_state: èµ·å§‹çŠ¶æ€
            n_episodes: æ¨¡æ‹Ÿå›åˆæ•°
            max_steps: æ¯å›åˆæœ€å¤§æ­¥æ•°
            figsize: å›¾å½¢å¤§å°
        """
        print(f"\nğŸ® æ¨¡æ‹Ÿ {n_episodes} ä¸ªå›åˆï¼Œæ¯å›åˆæœ€å¤š {max_steps} æ­¥")
        
        # è¿›è¡Œå¤šæ¬¡æ¨¡æ‹Ÿ
        all_returns = []
        all_lengths = []
        episode_data = []
        
        for episode in range(n_episodes):
            states_seq, rewards_seq = self.mrp.simulate_episode(start_state, max_steps)
            discounted_return = self.mrp.compute_discounted_return(rewards_seq)
            
            all_returns.append(discounted_return)
            all_lengths.append(len(rewards_seq))
            episode_data.append((states_seq, rewards_seq, discounted_return))
        
        # åˆ›å»ºå­å›¾
        fig = plt.figure(figsize=figsize)
        gs = fig.add_gridspec(3, 2, height_ratios=[1, 1, 1], width_ratios=[2, 1])
        
        # 1. å›åˆè½¨è¿¹å›¾
        ax1 = fig.add_subplot(gs[0, :])
        for i, (states_seq, rewards_seq, ret) in enumerate(episode_data[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ªå›åˆ
            steps = range(len(states_seq))
            state_indices = [self.mrp.state_to_index[s] for s in states_seq]
            ax1.plot(steps, state_indices, 'o-', alpha=0.7, label=f'å›åˆ{i+1} (G={ret:.2f})')
        
        ax1.set_xlabel('æ—¶é—´æ­¥')
        ax1.set_ylabel('çŠ¶æ€ç´¢å¼•')
        ax1.set_title(f'å‰10ä¸ªå›åˆçš„çŠ¶æ€è½¨è¿¹ (ä» "{start_state}" å¼€å§‹)')
        ax1.set_yticks(range(self.mrp.n_states))
        ax1.set_yticklabels(self.mrp.states)
        ax1.grid(True, alpha=0.3)
        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        
        # 2. æŠ˜æ‰£å›æŠ¥åˆ†å¸ƒ
        ax2 = fig.add_subplot(gs[1, 0])
        ax2.hist(all_returns, bins=min(10, n_episodes//2), alpha=0.7, color='skyblue', edgecolor='black')
        ax2.axvline(np.mean(all_returns), color='red', linestyle='--', label=f'å‡å€¼: {np.mean(all_returns):.3f}')
        ax2.set_xlabel('æŠ˜æ‰£å›æŠ¥ G')
        ax2.set_ylabel('é¢‘æ¬¡')
        ax2.set_title('æŠ˜æ‰£å›æŠ¥åˆ†å¸ƒ')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. å›åˆé•¿åº¦åˆ†å¸ƒ
        ax3 = fig.add_subplot(gs[1, 1])
        ax3.hist(all_lengths, bins=min(10, max(all_lengths)-min(all_lengths)+1), 
                alpha=0.7, color='lightgreen', edgecolor='black')
        ax3.axvline(np.mean(all_lengths), color='red', linestyle='--', label=f'å‡å€¼: {np.mean(all_lengths):.1f}')
        ax3.set_xlabel('å›åˆé•¿åº¦')
        ax3.set_ylabel('é¢‘æ¬¡')
        ax3.set_title('å›åˆé•¿åº¦åˆ†å¸ƒ')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. ç»Ÿè®¡æ‘˜è¦
        ax4 = fig.add_subplot(gs[2, :])
        ax4.axis('off')
        
        # è®¡ç®—ç†è®ºä»·å€¼ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        theoretical_value = None
        if self.mrp.analytical_solution is not None:
            start_idx = self.mrp.state_to_index[start_state]
            theoretical_value = self.mrp.analytical_solution[start_idx]
        elif self.mrp.iterative_solution is not None:
            start_idx = self.mrp.state_to_index[start_state]
            theoretical_value = self.mrp.iterative_solution[start_idx]
        
        stats_text = f"""
        æ¨¡æ‹Ÿç»Ÿè®¡æ‘˜è¦ (èµ·å§‹çŠ¶æ€: {start_state})
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        æŠ˜æ‰£å›æŠ¥ç»Ÿè®¡:
        â€¢ å¹³å‡å€¼: {np.mean(all_returns):.6f}
        â€¢ æ ‡å‡†å·®: {np.std(all_returns):.6f}
        â€¢ æœ€å°å€¼: {np.min(all_returns):.6f}
        â€¢ æœ€å¤§å€¼: {np.max(all_returns):.6f}
        
        å›åˆé•¿åº¦ç»Ÿè®¡:
        â€¢ å¹³å‡é•¿åº¦: {np.mean(all_lengths):.2f} æ­¥
        â€¢ æœ€çŸ­å›åˆ: {np.min(all_lengths)} æ­¥
        â€¢ æœ€é•¿å›åˆ: {np.max(all_lengths)} æ­¥
        
        ç†è®ºå¯¹æ¯”:
        â€¢ ç†è®ºä»·å€¼ V({start_state}): {f"{theoretical_value:.6f}" if theoretical_value is not None else "æœªè®¡ç®—"}
        â€¢ æ¨¡æ‹Ÿè¯¯å·®: {f"{abs(np.mean(all_returns) - theoretical_value):.6f}" if theoretical_value is not None else "N/A"}
        """
        
        ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, fontsize=11,
                verticalalignment='top', fontfamily='sans-serif',
                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        plt.show()
        
        return all_returns, all_lengths


def create_student_learning_mrp() -> MarkovRewardProcessDetailed:
    """
    åˆ›å»ºå­¦ç”Ÿå­¦ä¹ è¿‡ç¨‹çš„è¯¦ç»†MRPç¤ºä¾‹
    
    è¿™æ˜¯å¼ºåŒ–å­¦ä¹ æ•™å­¦ä¸­çš„ç»å…¸ç¤ºä¾‹ï¼Œå±•ç¤ºäº†å­¦ç”Ÿåœ¨ä¸åŒå­¦ä¹ çŠ¶æ€é—´çš„è½¬ç§»ã€‚
    """
    print("\n" + "="*80)
    print("ğŸ“š åˆ›å»ºç¤ºä¾‹ï¼šå­¦ç”Ÿå­¦ä¹ è¿‡ç¨‹é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹")
    print("="*80)
    
    # å®šä¹‰çŠ¶æ€å’Œæè¿°
    states = ['ä¸“å¿ƒå­¦ä¹ ', 'åˆ†å¿ƒ', 'ç¡è§‰', 'è€ƒè¯•']
    state_descriptions = {
        'ä¸“å¿ƒå­¦ä¹ ': 'å­¦ç”Ÿå…¨ç¥è´¯æ³¨åœ°å­¦ä¹ ï¼Œå¸æ”¶çŸ¥è¯†æ•ˆç‡é«˜',
        'åˆ†å¿ƒ': 'å­¦ç”Ÿæ³¨æ„åŠ›ä¸é›†ä¸­ï¼Œå­¦ä¹ æ•ˆæœå·®ï¼Œå®¹æ˜“å—å¹²æ‰°',
        'ç¡è§‰': 'å­¦ç”Ÿä¼‘æ¯çŠ¶æ€ï¼Œæ—¢ä¸å­¦ä¹ ä¹Ÿä¸å¨±ä¹',
        'è€ƒè¯•': 'å­¦ç”Ÿå‚åŠ è€ƒè¯•ï¼Œæ ¹æ®ä¹‹å‰çš„å­¦ä¹ çŠ¶æ€è·å¾—ç›¸åº”ç»“æœ'
    }
    
    # å®šä¹‰è½¬ç§»æ¦‚ç‡çŸ©é˜µ
    # ä¸“å¿ƒå­¦ä¹  -> [ä¸“å¿ƒå­¦ä¹ , åˆ†å¿ƒ, ç¡è§‰, è€ƒè¯•]
    # åˆ†å¿ƒ -> [ä¸“å¿ƒå­¦ä¹ , åˆ†å¿ƒ, ç¡è§‰, è€ƒè¯•]
    # ç¡è§‰ -> [ä¸“å¿ƒå­¦ä¹ , åˆ†å¿ƒ, ç¡è§‰, è€ƒè¯•]
    # è€ƒè¯• -> [ä¸“å¿ƒå­¦ä¹ , åˆ†å¿ƒ, ç¡è§‰, è€ƒè¯•] (è€ƒè¯•åé‡æ–°å¼€å§‹å­¦ä¹ å‘¨æœŸ)
    transition_matrix = np.array([
        [0.6, 0.2, 0.1, 0.1],  # ä»ä¸“å¿ƒå­¦ä¹ 
        [0.3, 0.3, 0.3, 0.1],  # ä»åˆ†å¿ƒ
        [0.2, 0.1, 0.6, 0.1],  # ä»ç¡è§‰
        [0.4, 0.2, 0.2, 0.2]   # ä»è€ƒè¯•ï¼ˆé‡æ–°å¼€å§‹ï¼‰
    ])
    
    # å®šä¹‰å¥–åŠ±å‡½æ•°
    rewards = np.array([15.0, -8.0, -2.0, 25.0])  # ä¸“å¿ƒå­¦ä¹ +15, åˆ†å¿ƒ-8, ç¡è§‰-2, è€ƒè¯•+25
    
    # æŠ˜æ‰£å› å­
    gamma = 0.9
    
    print("çŠ¶æ€å®šä¹‰:")
    for state, desc in state_descriptions.items():
        print(f"  â€¢ {state}: {desc}")
    
    print(f"\nå¥–åŠ±è®¾è®¡ç†å¿µ:")
    print(f"  â€¢ ä¸“å¿ƒå­¦ä¹  (+15): è·å¾—çŸ¥è¯†ï¼Œé•¿æœŸæ”¶ç›Šé«˜")
    print(f"  â€¢ åˆ†å¿ƒ (-8): æµªè´¹æ—¶é—´ï¼Œæœºä¼šæˆæœ¬é«˜")
    print(f"  â€¢ ç¡è§‰ (-2): è™½ç„¶å¿…è¦ï¼Œä½†å­¦ä¹ æ—¶é—´å‡å°‘")
    print(f"  â€¢ è€ƒè¯• (+25): æ£€éªŒå­¦ä¹ æˆæœï¼Œè·å¾—æˆå°±æ„Ÿ")
    
    # åˆ›å»ºMRP
    mrp = MarkovRewardProcessDetailed(states, transition_matrix, rewards, gamma, state_descriptions)
    
    return mrp


def demonstrate_mrp_concepts():
    """æ¼”ç¤ºé©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹çš„æ ¸å¿ƒæ¦‚å¿µ"""
    print("\n" + "ğŸ“" * 40)
    print("é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹ (MRP) æ ¸å¿ƒæ¦‚å¿µæ¼”ç¤º")
    print("ğŸ“" * 40)
    
    # åˆ›å»ºç¤ºä¾‹MRP
    mrp = create_student_learning_mrp()
    
    # æ‰“å°è¯¦ç»†ä¿¡æ¯
    mrp.print_detailed_summary()
    
    # è®¡ç®—ä»·å€¼å‡½æ•° - è§£æè§£
    print("\n" + "="*60)
    print("ğŸ§® ç¬¬ä¸€æ­¥ï¼šè®¡ç®—ä»·å€¼å‡½æ•° - è§£æè§£æ–¹æ³•")
    print("="*60)
    V_analytical = mrp.compute_value_function_analytical()
    
    # è®¡ç®—ä»·å€¼å‡½æ•° - è¿­ä»£è§£
    print("\n" + "="*60)
    print("ğŸ”„ ç¬¬äºŒæ­¥ï¼šè®¡ç®—ä»·å€¼å‡½æ•° - ä»·å€¼è¿­ä»£æ–¹æ³•")
    print("="*60)
    V_iterative = mrp.compute_value_function_iterative(max_iterations=100, tolerance=1e-10)
    
    # åˆ†æå¹³ç¨³åˆ†å¸ƒ
    print("\n" + "="*60)
    print("ğŸ“Š ç¬¬ä¸‰æ­¥ï¼šåˆ†æé©¬å°”å¯å¤«é“¾çš„å¹³ç¨³åˆ†å¸ƒ")
    print("="*60)
    stationary_dist = mrp.analyze_stationary_distribution()
    
    # åˆ›å»ºå¯è§†åŒ–
    print("\n" + "="*60)
    print("ğŸ“ˆ ç¬¬å››æ­¥ï¼šç”Ÿæˆå¯è§†åŒ–åˆ†æ")
    print("="*60)
    visualizer = MRPVisualizer(mrp)
    
    print("æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    # 1. çŠ¶æ€è½¬ç§»å›¾
    print("1. çŠ¶æ€è½¬ç§»å›¾")
    visualizer.plot_state_transition_graph()
    
    # 2. è½¬ç§»çŸ©é˜µçƒ­åŠ›å›¾
    print("2. è½¬ç§»æ¦‚ç‡çŸ©é˜µçƒ­åŠ›å›¾")
    visualizer.plot_transition_matrix_heatmap()
    
    # 3. ä»·å€¼å‡½æ•°æ¯”è¾ƒ
    print("3. ä»·å€¼å‡½æ•°æ¯”è¾ƒå›¾")
    visualizer.plot_value_function_comparison()
    
    # 4. å¥–åŠ±å‡½æ•°
    print("4. å¥–åŠ±å‡½æ•°å›¾")
    visualizer.plot_reward_function()
    
    # 5. å›åˆæ¨¡æ‹Ÿ
    print("5. å›åˆæ¨¡æ‹Ÿåˆ†æ")
    returns, lengths = visualizer.plot_episode_simulation('ä¸“å¿ƒå­¦ä¹ ', n_episodes=30, max_steps=20)
    
    # æœ€ç»ˆæ€»ç»“
    print("\n" + "="*80)
    print("ğŸ“‹ é©¬å°”å¯å¤«å¥–åŠ±è¿‡ç¨‹åˆ†ææ€»ç»“")
    print("="*80)
    
    print(f"\nğŸ¯ å…³é”®å‘ç°:")
    if V_analytical is not None:
        best_state = mrp.states[np.argmax(V_analytical)]
        worst_state = mrp.states[np.argmin(V_analytical)]
        print(f"  â€¢ æœ€ä¼˜çŠ¶æ€: {best_state} (V = {np.max(V_analytical):.3f})")
        print(f"  â€¢ æœ€å·®çŠ¶æ€: {worst_state} (V = {np.min(V_analytical):.3f})")
    
    if stationary_dist is not None:
        most_likely_state = mrp.states[np.argmax(stationary_dist)]
        print(f"  â€¢ é•¿æœŸæœ€å¯èƒ½çŠ¶æ€: {most_likely_state} (Ï€ = {np.max(stationary_dist):.3f})")
    
    print(f"\nğŸ“Š æ¨¡æ‹ŸéªŒè¯:")
    print(f"  â€¢ å¹³å‡æŠ˜æ‰£å›æŠ¥: {np.mean(returns):.3f}")
    print(f"  â€¢ å›æŠ¥æ ‡å‡†å·®: {np.std(returns):.3f}")
    print(f"  â€¢ å¹³å‡å›åˆé•¿åº¦: {np.mean(lengths):.1f} æ­¥")
    
    print(f"\nğŸ’¡ æ•™å­¦è¦ç‚¹:")
    print(f"  1. é©¬å°”å¯å¤«æ€§è´¨ï¼šæœªæ¥åªä¾èµ–å½“å‰çŠ¶æ€")
    print(f"  2. ä»·å€¼å‡½æ•°ï¼šè¡¡é‡çŠ¶æ€çš„é•¿æœŸä»·å€¼")
    print(f"  3. è´å°”æ›¼æ–¹ç¨‹ï¼šä»·å€¼å‡½æ•°çš„é€’å½’å…³ç³»")
    print(f"  4. æŠ˜æ‰£å› å­ï¼šå¹³è¡¡å³æ—¶å¥–åŠ±å’Œæœªæ¥å¥–åŠ±")
    print(f"  5. æ”¶æ•›æ€§ï¼šè¿­ä»£ç®—æ³•çš„æ•°å­¦ä¿è¯")


if __name__ == "__main__":
    # è¿è¡Œå®Œæ•´çš„MRPæ¦‚å¿µæ¼”ç¤º
    demonstrate_mrp_concepts()