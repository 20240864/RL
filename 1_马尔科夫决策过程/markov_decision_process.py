"""
é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹ (Markov Decision Process, MDP) è¯¦ç»†å®ç°

æœ¬æ–‡ä»¶æä¾›äº†MDPçš„å®Œæ•´å®ç°ï¼ŒåŒ…æ‹¬ï¼š
1. æ ¸å¿ƒMDPæ•°å­¦ç†è®ºçš„ä¸¥æ ¼å®ç°
2. ç­–ç•¥è¡¨ç¤ºå’Œè¯„ä¼°
3. ç­–ç•¥è¿­ä»£å’Œä»·å€¼è¿­ä»£ç®—æ³•
4. ä¸°å¯Œçš„å¯è§†åŒ–åŠŸèƒ½
5. è¯¦ç»†çš„æ•™å­¦ç¤ºä¾‹å’Œåˆ†æ

ä½œè€…ï¼šå¼ºåŒ–å­¦ä¹ æ•™å­¦ä»£ç 
ç‰ˆæœ¬ï¼š2.0
ç”¨é€”ï¼šæ·±å…¥ç†è§£é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹çš„æ•°å­¦åŸç†å’Œç®—æ³•å®ç°
"""

import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
import seaborn as sns
from typing import List, Dict, Tuple, Optional, Union, Callable
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class MarkovDecisionProcess:
    """
    é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹è¯¦ç»†å®ç°ç±»
    
    é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹æ˜¯ä¸€ä¸ªäº”å…ƒç»„ (S, A, P, R, Î³)ï¼š
    - S: çŠ¶æ€ç©ºé—´ (State Space)
    - A: åŠ¨ä½œç©ºé—´ (Action Space)
    - P: çŠ¶æ€è½¬ç§»æ¦‚ç‡å‡½æ•° P(s'|s,a)
    - R: å¥–åŠ±å‡½æ•° R(s,a,s') æˆ– R(s,a)
    - Î³: æŠ˜æ‰£å› å­ (Discount Factor)
    
    æ ¸å¿ƒæ¦‚å¿µï¼š
    1. ç­–ç•¥ Ï€(a|s): åœ¨çŠ¶æ€sä¸‹é€‰æ‹©åŠ¨ä½œaçš„æ¦‚ç‡
    2. çŠ¶æ€ä»·å€¼å‡½æ•° V^Ï€(s): åœ¨ç­–ç•¥Ï€ä¸‹çŠ¶æ€sçš„æœŸæœ›å›æŠ¥
    3. åŠ¨ä½œä»·å€¼å‡½æ•° Q^Ï€(s,a): åœ¨ç­–ç•¥Ï€ä¸‹çŠ¶æ€sæ‰§è¡ŒåŠ¨ä½œaçš„æœŸæœ›å›æŠ¥
    4. è´å°”æ›¼æ–¹ç¨‹ï¼šV^Ï€(s) = Î£_a Ï€(a|s) * Î£_{s'} P(s'|s,a) * [R(s,a,s') + Î³*V^Ï€(s')]
    """
    
    def __init__(self, states: List[str], actions: List[str], 
                 transition_probs: Dict[Tuple[str, str, str], float],
                 rewards: Dict[Tuple[str, str], float], gamma: float,
                 state_descriptions: Optional[Dict[str, str]] = None,
                 action_descriptions: Optional[Dict[str, str]] = None):
        """
        åˆå§‹åŒ–é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹
        
        å‚æ•°:
            states: çŠ¶æ€åç§°åˆ—è¡¨
            actions: åŠ¨ä½œåç§°åˆ—è¡¨
            transition_probs: è½¬ç§»æ¦‚ç‡å­—å…¸ {(s, a, s'): P(s'|s,a)}
            rewards: å¥–åŠ±å‡½æ•°å­—å…¸ {(s, a): R(s,a)}
            gamma: æŠ˜æ‰£å› å­ Î³ âˆˆ [0,1]
            state_descriptions: çŠ¶æ€æè¿°å­—å…¸ï¼ˆå¯é€‰ï¼‰
            action_descriptions: åŠ¨ä½œæè¿°å­—å…¸ï¼ˆå¯é€‰ï¼‰
        """
        self.states = states
        self.actions = actions
        self.n_states = len(states)
        self.n_actions = len(actions)
        self.state_to_index = {state: i for i, state in enumerate(states)}
        self.action_to_index = {action: i for i, action in enumerate(actions)}
        self.index_to_state = {i: state for i, state in enumerate(states)}
        self.index_to_action = {i: action for i, action in enumerate(actions)}
        
        self.gamma = gamma
        self.state_descriptions = state_descriptions or {}
        self.action_descriptions = action_descriptions or {}
        
        # æ„å»ºè½¬ç§»æ¦‚ç‡å¼ é‡å’Œå¥–åŠ±çŸ©é˜µ
        self._build_transition_tensor(transition_probs)
        self._build_reward_matrix(rewards)
        
        # éªŒè¯è¾“å…¥å‚æ•°
        self._validate_parameters()
        
        # å­˜å‚¨è®¡ç®—å†å²
        self.policy_iteration_history = []
        self.value_iteration_history = []
        self.current_policy = None
        self.current_value_function = None
        
        print(f"âœ… é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹åˆå§‹åŒ–å®Œæˆ")
        print(f"   çŠ¶æ€æ•°é‡: {self.n_states}")
        print(f"   åŠ¨ä½œæ•°é‡: {self.n_actions}")
        print(f"   æŠ˜æ‰£å› å­: {self.gamma}")
        print(f"   çŠ¶æ€åˆ—è¡¨: {self.states}")
        print(f"   åŠ¨ä½œåˆ—è¡¨: {self.actions}")
    
    def _build_transition_tensor(self, transition_probs: Dict[Tuple[str, str, str], float]):
        """æ„å»ºè½¬ç§»æ¦‚ç‡å¼ é‡ P[s,a,s'] = P(s'|s,a)"""
        self.P = np.zeros((self.n_states, self.n_actions, self.n_states))
        
        for (s, a, s_next), prob in transition_probs.items():
            s_idx = self.state_to_index[s]
            a_idx = self.action_to_index[a]
            s_next_idx = self.state_to_index[s_next]
            self.P[s_idx, a_idx, s_next_idx] = prob
    
    def _build_reward_matrix(self, rewards: Dict[Tuple[str, str], float]):
        """æ„å»ºå¥–åŠ±çŸ©é˜µ R[s,a] = R(s,a)"""
        self.R = np.zeros((self.n_states, self.n_actions))
        
        for (s, a), reward in rewards.items():
            s_idx = self.state_to_index[s]
            a_idx = self.action_to_index[a]
            self.R[s_idx, a_idx] = reward
    
    def _validate_parameters(self):
        """éªŒè¯è¾“å…¥å‚æ•°çš„æœ‰æ•ˆæ€§"""
        # æ£€æŸ¥è½¬ç§»æ¦‚ç‡å¼ é‡
        for s_idx in range(self.n_states):
            for a_idx in range(self.n_actions):
                prob_sum = np.sum(self.P[s_idx, a_idx, :])
                if not np.isclose(prob_sum, 1.0, rtol=1e-10):
                    state = self.states[s_idx]
                    action = self.actions[a_idx]
                    print(f"âš ï¸  è­¦å‘Š: çŠ¶æ€ {state} åŠ¨ä½œ {action} çš„è½¬ç§»æ¦‚ç‡å’Œä¸º {prob_sum:.6f}")
        
        # æ£€æŸ¥æ¦‚ç‡éè´Ÿ
        if np.any(self.P < 0):
            raise ValueError("è½¬ç§»æ¦‚ç‡ä¸èƒ½ä¸ºè´Ÿæ•°")
        
        # æ£€æŸ¥æŠ˜æ‰£å› å­
        if not 0 <= self.gamma <= 1:
            raise ValueError(f"æŠ˜æ‰£å› å­å¿…é¡»åœ¨[0,1]èŒƒå›´å†…, å®é™…: {self.gamma}")
    
    def create_random_policy(self) -> np.ndarray:
        """
        åˆ›å»ºéšæœºç­–ç•¥ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰
        
        è¿”å›:
            ç­–ç•¥çŸ©é˜µ Ï€[s,a] = Ï€(a|s)
        """
        policy = np.ones((self.n_states, self.n_actions)) / self.n_actions
        return policy
    
    def create_deterministic_policy(self, action_mapping: Dict[str, str]) -> np.ndarray:
        """
        åˆ›å»ºç¡®å®šæ€§ç­–ç•¥
        
        å‚æ•°:
            action_mapping: çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„ {state: action}
            
        è¿”å›:
            ç­–ç•¥çŸ©é˜µ Ï€[s,a] = Ï€(a|s)
        """
        policy = np.zeros((self.n_states, self.n_actions))
        
        for state, action in action_mapping.items():
            s_idx = self.state_to_index[state]
            a_idx = self.action_to_index[action]
            policy[s_idx, a_idx] = 1.0
        
        return policy
    
    def policy_evaluation(self, policy: np.ndarray, max_iterations: int = 1000, 
                         tolerance: float = 1e-8, verbose: bool = True) -> np.ndarray:
        """
        ç­–ç•¥è¯„ä¼°ï¼šè®¡ç®—ç»™å®šç­–ç•¥çš„çŠ¶æ€ä»·å€¼å‡½æ•°
        
        ä½¿ç”¨è¿­ä»£æ–¹æ³•æ±‚è§£è´å°”æ›¼æœŸæœ›æ–¹ç¨‹ï¼š
        V^Ï€(s) = Î£_a Ï€(a|s) * Î£_{s'} P(s'|s,a) * [R(s,a) + Î³*V^Ï€(s')]
        
        å‚æ•°:
            policy: ç­–ç•¥çŸ©é˜µ Ï€[s,a]
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            tolerance: æ”¶æ•›å®¹å¿åº¦
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        è¿”å›:
            çŠ¶æ€ä»·å€¼å‡½æ•° V^Ï€
        """
        if verbose:
            print(f"\nğŸ” ç­–ç•¥è¯„ä¼°å¼€å§‹")
            print(f"   æœ€å¤§è¿­ä»£æ¬¡æ•°: {max_iterations}")
            print(f"   æ”¶æ•›å®¹å¿åº¦: {tolerance}")
        
        V = np.zeros(self.n_states)
        
        for iteration in range(max_iterations):
            V_old = V.copy()
            
            for s in range(self.n_states):
                v_new = 0
                for a in range(self.n_actions):
                    action_value = 0
                    for s_next in range(self.n_states):
                        action_value += self.P[s, a, s_next] * (self.R[s, a] + self.gamma * V_old[s_next])
                    v_new += policy[s, a] * action_value
                V[s] = v_new
            
            # æ£€æŸ¥æ”¶æ•›
            delta = np.max(np.abs(V - V_old))
            
            if verbose and (iteration < 10 or iteration % 100 == 0):
                print(f"   è¿­ä»£ {iteration:3d}: æœ€å¤§å˜åŒ–é‡ = {delta:.8f}")
            
            if delta < tolerance:
                if verbose:
                    print(f"   âœ… åœ¨ç¬¬ {iteration} æ¬¡è¿­ä»£åæ”¶æ•›")
                break
        else:
            if verbose:
                print(f"   âš ï¸  è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œæœªå®Œå…¨æ”¶æ•›")
        
        return V
    
    def policy_improvement(self, V: np.ndarray) -> Tuple[np.ndarray, bool]:
        """
        ç­–ç•¥æ”¹è¿›ï¼šåŸºäºä»·å€¼å‡½æ•°ç”Ÿæˆè´ªå©ªç­–ç•¥
        
        æ–°ç­–ç•¥ï¼šÏ€'(s) = argmax_a Î£_{s'} P(s'|s,a) * [R(s,a) + Î³*V(s')]
        
        å‚æ•°:
            V: çŠ¶æ€ä»·å€¼å‡½æ•°
            
        è¿”å›:
            (æ–°ç­–ç•¥, æ˜¯å¦ç¨³å®š)
        """
        new_policy = np.zeros((self.n_states, self.n_actions))
        policy_stable = True
        
        for s in range(self.n_states):
            # è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„Qå€¼
            q_values = np.zeros(self.n_actions)
            for a in range(self.n_actions):
                for s_next in range(self.n_states):
                    q_values[a] += self.P[s, a, s_next] * (self.R[s, a] + self.gamma * V[s_next])
            
            # é€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼ˆè´ªå©ªç­–ç•¥ï¼‰
            best_actions = np.where(q_values == np.max(q_values))[0]
            
            # å¦‚æœæœ‰å¤šä¸ªæœ€ä¼˜åŠ¨ä½œï¼Œå¹³å‡åˆ†é…æ¦‚ç‡
            for a in best_actions:
                new_policy[s, a] = 1.0 / len(best_actions)
        
        return new_policy, policy_stable
    
    def policy_iteration(self, initial_policy: Optional[np.ndarray] = None, 
                        max_iterations: int = 100, tolerance: float = 1e-8,
                        verbose: bool = True) -> Tuple[np.ndarray, np.ndarray]:
        """
        ç­–ç•¥è¿­ä»£ç®—æ³•
        
        äº¤æ›¿è¿›è¡Œç­–ç•¥è¯„ä¼°å’Œç­–ç•¥æ”¹è¿›ï¼Œç›´åˆ°ç­–ç•¥æ”¶æ•›
        
        å‚æ•°:
            initial_policy: åˆå§‹ç­–ç•¥ï¼ˆé»˜è®¤ä¸ºéšæœºç­–ç•¥ï¼‰
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            tolerance: æ”¶æ•›å®¹å¿åº¦
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        è¿”å›:
            (æœ€ä¼˜ç­–ç•¥, æœ€ä¼˜ä»·å€¼å‡½æ•°)
        """
        if verbose:
            print(f"\nğŸ”„ ç­–ç•¥è¿­ä»£ç®—æ³•å¼€å§‹")
            print("=" * 60)
        
        # åˆå§‹åŒ–ç­–ç•¥
        if initial_policy is None:
            policy = self.create_random_policy()
        else:
            policy = initial_policy.copy()
        
        self.policy_iteration_history = []
        
        for iteration in range(max_iterations):
            if verbose:
                print(f"\nç­–ç•¥è¿­ä»£ - ç¬¬ {iteration + 1} è½®:")
            
            # ç­–ç•¥è¯„ä¼°
            V = self.policy_evaluation(policy, verbose=False)
            
            # ç­–ç•¥æ”¹è¿›
            new_policy, policy_stable = self.policy_improvement(V)
            
            # æ£€æŸ¥ç­–ç•¥å˜åŒ–
            policy_change = np.max(np.abs(new_policy - policy))
            self.policy_iteration_history.append({
                'iteration': iteration,
                'policy_change': policy_change,
                'value_function': V.copy(),
                'policy': new_policy.copy()
            })
            
            if verbose:
                print(f"   ç­–ç•¥å˜åŒ–é‡: {policy_change:.8f}")
                print(f"   ä»·å€¼å‡½æ•°: {V}")
            
            # æ£€æŸ¥æ”¶æ•›
            if policy_change < tolerance:
                if verbose:
                    print(f"   âœ… ç­–ç•¥åœ¨ç¬¬ {iteration + 1} è½®åæ”¶æ•›")
                break
            
            policy = new_policy
        else:
            if verbose:
                print(f"   âš ï¸  è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œç­–ç•¥å¯èƒ½æœªå®Œå…¨æ”¶æ•›")
        
        self.current_policy = policy
        self.current_value_function = V
        
        if verbose:
            print(f"\nğŸ¯ æœ€ä¼˜ç­–ç•¥:")
            self._print_policy(policy)
        
        return policy, V
    
    def value_iteration(self, max_iterations: int = 1000, tolerance: float = 1e-8,
                       verbose: bool = True) -> Tuple[np.ndarray, np.ndarray]:
        """
        ä»·å€¼è¿­ä»£ç®—æ³•
        
        ç›´æ¥è¿­ä»£è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ï¼š
        V*(s) = max_a Î£_{s'} P(s'|s,a) * [R(s,a) + Î³*V*(s')]
        
        å‚æ•°:
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            tolerance: æ”¶æ•›å®¹å¿åº¦
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        è¿”å›:
            (æœ€ä¼˜ç­–ç•¥, æœ€ä¼˜ä»·å€¼å‡½æ•°)
        """
        if verbose:
            print(f"\nâš¡ ä»·å€¼è¿­ä»£ç®—æ³•å¼€å§‹")
            print("=" * 60)
            print(f"   æœ€å¤§è¿­ä»£æ¬¡æ•°: {max_iterations}")
            print(f"   æ”¶æ•›å®¹å¿åº¦: {tolerance}")
        
        V = np.zeros(self.n_states)
        self.value_iteration_history = []
        
        for iteration in range(max_iterations):
            V_old = V.copy()
            
            for s in range(self.n_states):
                # è®¡ç®—æ‰€æœ‰åŠ¨ä½œçš„Qå€¼
                q_values = np.zeros(self.n_actions)
                for a in range(self.n_actions):
                    for s_next in range(self.n_states):
                        q_values[a] += self.P[s, a, s_next] * (self.R[s, a] + self.gamma * V_old[s_next])
                
                # é€‰æ‹©æœ€å¤§Qå€¼
                V[s] = np.max(q_values)
            
            # è®¡ç®—å˜åŒ–é‡
            delta = np.max(np.abs(V - V_old))
            self.value_iteration_history.append({
                'iteration': iteration,
                'delta': delta,
                'value_function': V.copy()
            })
            
            if verbose and (iteration < 10 or iteration % 100 == 0):
                print(f"   è¿­ä»£ {iteration:3d}: æœ€å¤§å˜åŒ–é‡ = {delta:.8f}")
            
            # æ£€æŸ¥æ”¶æ•›
            if delta < tolerance:
                if verbose:
                    print(f"   âœ… åœ¨ç¬¬ {iteration} æ¬¡è¿­ä»£åæ”¶æ•›")
                break
        else:
            if verbose:
                print(f"   âš ï¸  è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œæœªå®Œå…¨æ”¶æ•›")
        
        # æå–æœ€ä¼˜ç­–ç•¥
        optimal_policy, _ = self.policy_improvement(V)
        
        self.current_policy = optimal_policy
        self.current_value_function = V
        
        if verbose:
            print(f"\nğŸ¯ æœ€ä¼˜ç­–ç•¥:")
            self._print_policy(optimal_policy)
            print(f"\nğŸ’° æœ€ä¼˜ä»·å€¼å‡½æ•°:")
            for i, (state, value) in enumerate(zip(self.states, V)):
                print(f"   V*({state}) = {value:.6f}")
        
        return optimal_policy, V
    
    def compute_q_function(self, policy: np.ndarray) -> np.ndarray:
        """
        è®¡ç®—åŠ¨ä½œä»·å€¼å‡½æ•° Q^Ï€(s,a)
        
        Q^Ï€(s,a) = Î£_{s'} P(s'|s,a) * [R(s,a) + Î³*V^Ï€(s')]
        
        å‚æ•°:
            policy: ç­–ç•¥çŸ©é˜µ
            
        è¿”å›:
            åŠ¨ä½œä»·å€¼å‡½æ•° Q[s,a]
        """
        V = self.policy_evaluation(policy, verbose=False)
        Q = np.zeros((self.n_states, self.n_actions))
        
        for s in range(self.n_states):
            for a in range(self.n_actions):
                for s_next in range(self.n_states):
                    Q[s, a] += self.P[s, a, s_next] * (self.R[s, a] + self.gamma * V[s_next])
        
        return Q
    
    def simulate_episode(self, policy: np.ndarray, start_state: str, 
                        max_steps: int = 100, random_seed: Optional[int] = None) -> Tuple[List[str], List[str], List[float]]:
        """
        æ ¹æ®ç»™å®šç­–ç•¥æ¨¡æ‹Ÿä¸€ä¸ªå›åˆ
        
        å‚æ•°:
            policy: ç­–ç•¥çŸ©é˜µ
            start_state: èµ·å§‹çŠ¶æ€
            max_steps: æœ€å¤§æ­¥æ•°
            random_seed: éšæœºç§å­
            
        è¿”å›:
            (çŠ¶æ€åºåˆ—, åŠ¨ä½œåºåˆ—, å¥–åŠ±åºåˆ—)
        """
        if random_seed is not None:
            np.random.seed(random_seed)
        
        states_sequence = [start_state]
        actions_sequence = []
        rewards_sequence = []
        
        current_state_idx = self.state_to_index[start_state]
        
        for step in range(max_steps):
            # æ ¹æ®ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
            action_probs = policy[current_state_idx]
            action_idx = np.random.choice(self.n_actions, p=action_probs)
            action = self.actions[action_idx]
            actions_sequence.append(action)
            
            # è·å¾—å¥–åŠ±
            reward = self.R[current_state_idx, action_idx]
            rewards_sequence.append(reward)
            
            # çŠ¶æ€è½¬ç§»
            next_state_probs = self.P[current_state_idx, action_idx]
            next_state_idx = np.random.choice(self.n_states, p=next_state_probs)
            next_state = self.states[next_state_idx]
            states_sequence.append(next_state)
            
            current_state_idx = next_state_idx
        
        return states_sequence, actions_sequence, rewards_sequence
    
    def _print_policy(self, policy: np.ndarray):
        """æ‰“å°ç­–ç•¥çš„å¯è¯»æ ¼å¼"""
        print("   çŠ¶æ€ -> åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ:")
        for s_idx, state in enumerate(self.states):
            action_probs = policy[s_idx]
            prob_str = ", ".join([f"{action}:{prob:.3f}" for action, prob in zip(self.actions, action_probs) if prob > 0.001])
            print(f"     {state}: {prob_str}")
    
    def print_detailed_summary(self):
        """æ‰“å°è¯¦ç»†çš„MDPä¿¡æ¯æ‘˜è¦"""
        print("\n" + "=" * 80)
        print("é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹ (MDP) è¯¦ç»†ä¿¡æ¯æ‘˜è¦")
        print("=" * 80)
        
        print(f"\nğŸ“‹ åŸºæœ¬ä¿¡æ¯:")
        print(f"   çŠ¶æ€ç©ºé—´å¤§å°: {self.n_states}")
        print(f"   åŠ¨ä½œç©ºé—´å¤§å°: {self.n_actions}")
        print(f"   çŠ¶æ€åˆ—è¡¨: {self.states}")
        print(f"   åŠ¨ä½œåˆ—è¡¨: {self.actions}")
        print(f"   æŠ˜æ‰£å› å­ Î³: {self.gamma}")
        
        if self.state_descriptions:
            print(f"\nğŸ“ çŠ¶æ€æè¿°:")
            for state, desc in self.state_descriptions.items():
                print(f"   {state}: {desc}")
        
        if self.action_descriptions:
            print(f"\nğŸ® åŠ¨ä½œæè¿°:")
            for action, desc in self.action_descriptions.items():
                print(f"   {action}: {desc}")
        
        print(f"\nğŸ¯ å¥–åŠ±å‡½æ•° R(s,a):")
        for s_idx, state in enumerate(self.states):
            for a_idx, action in enumerate(self.actions):
                reward = self.R[s_idx, a_idx]
                if reward != 0:  # åªæ˜¾ç¤ºéé›¶å¥–åŠ±
                    print(f"   R({state}, {action}) = {reward:8.3f}")
        
        print(f"\nğŸ”„ çŠ¶æ€è½¬ç§»æ¦‚ç‡ P(s'|s,a) (åªæ˜¾ç¤ºéé›¶æ¦‚ç‡):")
        for s_idx, state in enumerate(self.states):
            for a_idx, action in enumerate(self.actions):
                transitions = []
                for s_next_idx, next_state in enumerate(self.states):
                    prob = self.P[s_idx, a_idx, s_next_idx]
                    if prob > 0.001:
                        transitions.append(f"{next_state}:{prob:.3f}")
                if transitions:
                    print(f"   P(Â·|{state},{action}) = {{{', '.join(transitions)}}}")


class MDPVisualizer:
    """é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹å¯è§†åŒ–ç±»"""
    
    def __init__(self, mdp: MarkovDecisionProcess):
        """
        åˆå§‹åŒ–å¯è§†åŒ–å™¨
        
        å‚æ•°:
            mdp: é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹å®ä¾‹
        """
        self.mdp = mdp
        plt.style.use('default')
        # é‡æ–°è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œç¡®ä¿åœ¨styleè®¾ç½®åç”Ÿæ•ˆ
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
    
    def plot_policy_visualization(self, policy: np.ndarray, title: str = "ç­–ç•¥å¯è§†åŒ–", 
                                 figsize: Tuple[int, int] = (12, 8)):
        """
        ç»˜åˆ¶ç­–ç•¥å¯è§†åŒ–å›¾
        
        å‚æ•°:
            policy: ç­–ç•¥çŸ©é˜µ
            title: å›¾æ ‡é¢˜
            figsize: å›¾å½¢å¤§å°
        """
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)
        
        # å·¦å›¾ï¼šç­–ç•¥çƒ­åŠ›å›¾
        im1 = ax1.imshow(policy, cmap='Blues', aspect='auto')
        ax1.set_xticks(range(self.mdp.n_actions))
        ax1.set_yticks(range(self.mdp.n_states))
        ax1.set_xticklabels(self.mdp.actions)
        ax1.set_yticklabels(self.mdp.states)
        ax1.set_xlabel('åŠ¨ä½œ')
        ax1.set_ylabel('çŠ¶æ€')
        ax1.set_title('ç­–ç•¥æ¦‚ç‡çŸ©é˜µ Ï€(a|s)')
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for i in range(self.mdp.n_states):
            for j in range(self.mdp.n_actions):
                prob = policy[i, j]
                color = 'white' if prob > 0.5 else 'black'
                ax1.text(j, i, f'{prob:.2f}', ha='center', va='center', 
                        color=color, fontweight='bold')
        
        plt.colorbar(im1, ax=ax1, label='é€‰æ‹©æ¦‚ç‡')
        
        # å³å›¾ï¼šç¡®å®šæ€§ç­–ç•¥å›¾ï¼ˆæ˜¾ç¤ºæ¯ä¸ªçŠ¶æ€çš„æœ€ä¼˜åŠ¨ä½œï¼‰
        deterministic_actions = np.argmax(policy, axis=1)
        colors = plt.cm.Set3(np.linspace(0, 1, self.mdp.n_actions))
        
        bars = ax2.bar(range(self.mdp.n_states), [1] * self.mdp.n_states, 
                      color=[colors[a] for a in deterministic_actions])
        
        ax2.set_xticks(range(self.mdp.n_states))
        ax2.set_xticklabels(self.mdp.states, rotation=45)
        ax2.set_ylabel('æœ€ä¼˜åŠ¨ä½œ')
        ax2.set_title('ç¡®å®šæ€§ç­–ç•¥ (æœ€ä¼˜åŠ¨ä½œ)')
        ax2.set_ylim(0, 1.2)
        
        # æ·»åŠ åŠ¨ä½œæ ‡ç­¾
        for i, (bar, action_idx) in enumerate(zip(bars, deterministic_actions)):
            action = self.mdp.actions[action_idx]
            ax2.text(bar.get_x() + bar.get_width()/2., 0.5, action,
                    ha='center', va='center', fontweight='bold', rotation=90)
        
        # æ·»åŠ å›¾ä¾‹
        legend_elements = [plt.Rectangle((0,0),1,1, facecolor=colors[i], label=action) 
                          for i, action in enumerate(self.mdp.actions)]
        ax2.legend(handles=legend_elements, loc='upper right')
        
        plt.suptitle(title, fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
    
    def plot_value_function(self, V: np.ndarray, title: str = "çŠ¶æ€ä»·å€¼å‡½æ•°", 
                           figsize: Tuple[int, int] = (10, 6)):
        """
        ç»˜åˆ¶çŠ¶æ€ä»·å€¼å‡½æ•°
        
        å‚æ•°:
            V: çŠ¶æ€ä»·å€¼å‡½æ•°
            title: å›¾æ ‡é¢˜
            figsize: å›¾å½¢å¤§å°
        """
        fig, ax = plt.subplots(figsize=figsize)
        
        # åˆ›å»ºé¢œè‰²æ˜ å°„
        colors = ['red' if v < 0 else 'green' if v > 0 else 'gray' for v in V]
        
        bars = ax.bar(self.mdp.states, V, color=colors, alpha=0.7)
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for bar, value in zip(bars, V):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + (0.5 if height >= 0 else -0.5),
                   f'{value:.3f}', ha='center', va='bottom' if height >= 0 else 'top',
                   fontweight='bold')
        
        ax.set_xlabel('çŠ¶æ€')
        ax.set_ylabel('ä»·å€¼å‡½æ•° V(s)')
        ax.set_title(title)
        ax.grid(True, alpha=0.3)
        ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()
    
    def plot_q_function(self, Q: np.ndarray, title: str = "åŠ¨ä½œä»·å€¼å‡½æ•°", 
                       figsize: Tuple[int, int] = (12, 8)):
        """
        ç»˜åˆ¶åŠ¨ä½œä»·å€¼å‡½æ•°çƒ­åŠ›å›¾
        
        å‚æ•°:
            Q: åŠ¨ä½œä»·å€¼å‡½æ•°çŸ©é˜µ
            title: å›¾æ ‡é¢˜
            figsize: å›¾å½¢å¤§å°
        """
        fig, ax = plt.subplots(figsize=figsize)
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        im = ax.imshow(Q, cmap='RdYlGn', aspect='auto')
        
        # è®¾ç½®åæ ‡è½´
        ax.set_xticks(range(self.mdp.n_actions))
        ax.set_yticks(range(self.mdp.n_states))
        ax.set_xticklabels(self.mdp.actions)
        ax.set_yticklabels(self.mdp.states)
        ax.set_xlabel('åŠ¨ä½œ')
        ax.set_ylabel('çŠ¶æ€')
        ax.set_title(title + ' Q(s,a)')
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for i in range(self.mdp.n_states):
            for j in range(self.mdp.n_actions):
                value = Q[i, j]
                color = 'white' if abs(value) > np.max(np.abs(Q)) * 0.5 else 'black'
                ax.text(j, i, f'{value:.2f}', ha='center', va='center', 
                       color=color, fontweight='bold')
        
        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(im, ax=ax)
        cbar.set_label('Qå€¼', rotation=270, labelpad=20)
        
        plt.tight_layout()
        plt.show()
    
    def plot_convergence_history(self, figsize: Tuple[int, int] = (15, 6)):
        """
        ç»˜åˆ¶ç®—æ³•æ”¶æ•›å†å²
        
        å‚æ•°:
            figsize: å›¾å½¢å¤§å°
        """
        fig, axes = plt.subplots(1, 2, figsize=figsize)
        
        # ç­–ç•¥è¿­ä»£æ”¶æ•›å†å²
        if self.mdp.policy_iteration_history:
            iterations = [h['iteration'] for h in self.mdp.policy_iteration_history]
            policy_changes = [h['policy_change'] for h in self.mdp.policy_iteration_history]
            
            axes[0].semilogy(iterations, policy_changes, 'bo-', linewidth=2, markersize=6)
            axes[0].set_xlabel('è¿­ä»£æ¬¡æ•°')
            axes[0].set_ylabel('ç­–ç•¥å˜åŒ–é‡ (å¯¹æ•°å°ºåº¦)')
            axes[0].set_title('ç­–ç•¥è¿­ä»£æ”¶æ•›å†å²')
            axes[0].grid(True, alpha=0.3)
            
            if policy_changes:
                final_change = policy_changes[-1]
                axes[0].axhline(y=final_change, color='red', linestyle='--', alpha=0.7)
                axes[0].text(len(iterations)*0.7, final_change*2, 
                           f'æœ€ç»ˆå˜åŒ–: {final_change:.2e}', fontsize=10)
        else:
            axes[0].text(0.5, 0.5, 'æ— ç­–ç•¥è¿­ä»£å†å²', ha='center', va='center', 
                        transform=axes[0].transAxes, fontsize=12)
            axes[0].set_title('ç­–ç•¥è¿­ä»£æ”¶æ•›å†å²')
        
        # ä»·å€¼è¿­ä»£æ”¶æ•›å†å²
        if self.mdp.value_iteration_history:
            iterations = [h['iteration'] for h in self.mdp.value_iteration_history]
            deltas = [h['delta'] for h in self.mdp.value_iteration_history]
            
            axes[1].semilogy(iterations, deltas, 'ro-', linewidth=2, markersize=6)
            axes[1].set_xlabel('è¿­ä»£æ¬¡æ•°')
            axes[1].set_ylabel('æœ€å¤§å˜åŒ–é‡ (å¯¹æ•°å°ºåº¦)')
            axes[1].set_title('ä»·å€¼è¿­ä»£æ”¶æ•›å†å²')
            axes[1].grid(True, alpha=0.3)
            
            if deltas:
                final_delta = deltas[-1]
                axes[1].axhline(y=final_delta, color='red', linestyle='--', alpha=0.7)
                axes[1].text(len(iterations)*0.7, final_delta*2, 
                           f'æœ€ç»ˆè¯¯å·®: {final_delta:.2e}', fontsize=10)
        else:
            axes[1].text(0.5, 0.5, 'æ— ä»·å€¼è¿­ä»£å†å²', ha='center', va='center', 
                        transform=axes[1].transAxes, fontsize=12)
            axes[1].set_title('ä»·å€¼è¿­ä»£æ”¶æ•›å†å²')
        
        plt.tight_layout()
        plt.show()
    
    def plot_episode_analysis(self, policy: np.ndarray, start_state: str, 
                             n_episodes: int = 20, max_steps: int = 15,
                             figsize: Tuple[int, int] = (15, 10)):
        """
        ç»˜åˆ¶å›åˆåˆ†æå›¾
        
        å‚æ•°:
            policy: ç­–ç•¥çŸ©é˜µ
            start_state: èµ·å§‹çŠ¶æ€
            n_episodes: æ¨¡æ‹Ÿå›åˆæ•°
            max_steps: æ¯å›åˆæœ€å¤§æ­¥æ•°
            figsize: å›¾å½¢å¤§å°
        """
        print(f"\nğŸ® æ¨¡æ‹Ÿ {n_episodes} ä¸ªå›åˆï¼Œæ¯å›åˆæœ€å¤š {max_steps} æ­¥")
        
        # è¿›è¡Œå¤šæ¬¡æ¨¡æ‹Ÿ
        all_returns = []
        all_lengths = []
        episode_data = []
        
        for episode in range(n_episodes):
            states_seq, actions_seq, rewards_seq = self.mdp.simulate_episode(
                policy, start_state, max_steps)
            
            # è®¡ç®—æŠ˜æ‰£å›æŠ¥
            discounted_return = sum(reward * (self.mdp.gamma ** t) 
                                  for t, reward in enumerate(rewards_seq))
            
            all_returns.append(discounted_return)
            all_lengths.append(len(rewards_seq))
            episode_data.append((states_seq, actions_seq, rewards_seq, discounted_return))
        
        # åˆ›å»ºå­å›¾
        fig = plt.figure(figsize=figsize)
        gs = fig.add_gridspec(3, 2, height_ratios=[1.5, 1, 1])
        
        # 1. çŠ¶æ€-åŠ¨ä½œè½¨è¿¹å›¾
        ax1 = fig.add_subplot(gs[0, :])
        for i, (states_seq, actions_seq, rewards_seq, ret) in enumerate(episode_data[:8]):
            steps = range(len(states_seq)-1)  # åŠ¨ä½œåºåˆ—æ¯”çŠ¶æ€åºåˆ—å°‘1
            state_indices = [self.mdp.state_to_index[s] for s in states_seq[:-1]]
            
            # ç»˜åˆ¶çŠ¶æ€è½¨è¿¹
            ax1.plot(steps, state_indices, 'o-', alpha=0.7, 
                    label=f'å›åˆ{i+1} (G={ret:.2f})')
            
            # æ ‡æ³¨åŠ¨ä½œ
            for j, (step, action) in enumerate(zip(steps, actions_seq)):
                if j % 2 == 0:  # åªæ ‡æ³¨éƒ¨åˆ†åŠ¨ä½œä»¥é¿å…æ‹¥æŒ¤
                    ax1.annotate(action, (step, state_indices[j]), 
                               xytext=(5, 5), textcoords='offset points',
                               fontsize=8, alpha=0.7)
        
        ax1.set_xlabel('æ—¶é—´æ­¥')
        ax1.set_ylabel('çŠ¶æ€')
        ax1.set_title(f'å‰8ä¸ªå›åˆçš„çŠ¶æ€-åŠ¨ä½œè½¨è¿¹ (ä» "{start_state}" å¼€å§‹)')
        ax1.set_yticks(range(self.mdp.n_states))
        ax1.set_yticklabels(self.mdp.states)
        ax1.grid(True, alpha=0.3)
        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        
        # 2. æŠ˜æ‰£å›æŠ¥åˆ†å¸ƒ
        ax2 = fig.add_subplot(gs[1, 0])
        ax2.hist(all_returns, bins=min(10, n_episodes//2), alpha=0.7, 
                color='skyblue', edgecolor='black')
        ax2.axvline(np.mean(all_returns), color='red', linestyle='--', 
                   label=f'å‡å€¼: {np.mean(all_returns):.3f}')
        ax2.set_xlabel('æŠ˜æ‰£å›æŠ¥')
        ax2.set_ylabel('é¢‘æ¬¡')
        ax2.set_title('æŠ˜æ‰£å›æŠ¥åˆ†å¸ƒ')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. å›åˆé•¿åº¦åˆ†å¸ƒ
        ax3 = fig.add_subplot(gs[1, 1])
        ax3.hist(all_lengths, bins=min(10, max(all_lengths)-min(all_lengths)+1), 
                alpha=0.7, color='lightgreen', edgecolor='black')
        ax3.axvline(np.mean(all_lengths), color='red', linestyle='--', 
                   label=f'å‡å€¼: {np.mean(all_lengths):.1f}')
        ax3.set_xlabel('å›åˆé•¿åº¦')
        ax3.set_ylabel('é¢‘æ¬¡')
        ax3.set_title('å›åˆé•¿åº¦åˆ†å¸ƒ')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. ç»Ÿè®¡æ‘˜è¦
        ax4 = fig.add_subplot(gs[2, :])
        ax4.axis('off')
        
        # è®¡ç®—ç†è®ºä»·å€¼
        theoretical_value = None
        if self.mdp.current_value_function is not None:
            start_idx = self.mdp.state_to_index[start_state]
            theoretical_value = self.mdp.current_value_function[start_idx]
        
        theoretical_value_str = f"{theoretical_value:.6f}" if theoretical_value is not None else "æœªè®¡ç®—"
        simulation_error_str = f"{abs(np.mean(all_returns) - theoretical_value):.6f}" if theoretical_value is not None else "N/A"
        
        stats_text = f"""
        å›åˆæ¨¡æ‹Ÿç»Ÿè®¡æ‘˜è¦ (èµ·å§‹çŠ¶æ€: {start_state})
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        æŠ˜æ‰£å›æŠ¥ç»Ÿè®¡:                          å›åˆé•¿åº¦ç»Ÿè®¡:
        â€¢ å¹³å‡å€¼: {np.mean(all_returns):8.4f}              â€¢ å¹³å‡é•¿åº¦: {np.mean(all_lengths):6.2f} æ­¥
        â€¢ æ ‡å‡†å·®: {np.std(all_returns):8.4f}              â€¢ æœ€çŸ­å›åˆ: {np.min(all_lengths):6d} æ­¥
        â€¢ æœ€å°å€¼: {np.min(all_returns):8.4f}              â€¢ æœ€é•¿å›åˆ: {np.max(all_lengths):6d} æ­¥
        â€¢ æœ€å¤§å€¼: {np.max(all_returns):8.4f}
        
        ç†è®ºå¯¹æ¯”:
        â€¢ ç†è®ºä»·å€¼ V({start_state}): {theoretical_value_str}
        â€¢ æ¨¡æ‹Ÿè¯¯å·®: {simulation_error_str}
        """
        
        ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, fontsize=11,
                verticalalignment='top', fontfamily='sans-serif',
                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        plt.show()
        
        return all_returns, all_lengths


def create_grid_world_mdp() -> MarkovDecisionProcess:
    """
    åˆ›å»ºç½‘æ ¼ä¸–ç•ŒMDPç¤ºä¾‹
    
    è¿™æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­çš„ç»å…¸ç¤ºä¾‹ï¼Œæ™ºèƒ½ä½“åœ¨4x4ç½‘æ ¼ä¸­å¯»æ‰¾å®è—å¹¶é¿å¼€é™·é˜±ã€‚
    """
    print("\n" + "="*80)
    print("ğŸ—ºï¸  åˆ›å»ºç¤ºä¾‹ï¼šç½‘æ ¼ä¸–ç•Œé©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹")
    print("="*80)
    
    # å®šä¹‰çŠ¶æ€ï¼ˆ4x4ç½‘æ ¼ï¼‰
    states = []
    for i in range(4):
        for j in range(4):
            states.append(f"({i},{j})")
    
    # å®šä¹‰åŠ¨ä½œ
    actions = ['ä¸Š', 'ä¸‹', 'å·¦', 'å³']
    
    # çŠ¶æ€æè¿°
    state_descriptions = {
        '(0,0)': 'èµ·å§‹ä½ç½®',
        '(0,3)': 'å®è—ä½ç½® (+10å¥–åŠ±)',
        '(1,3)': 'é™·é˜±ä½ç½® (-10å¥–åŠ±)',
        '(3,3)': 'ç»ˆç‚¹ä½ç½®'
    }
    
    # åŠ¨ä½œæè¿°
    action_descriptions = {
        'ä¸Š': 'å‘ä¸Šç§»åŠ¨ä¸€æ ¼',
        'ä¸‹': 'å‘ä¸‹ç§»åŠ¨ä¸€æ ¼',
        'å·¦': 'å‘å·¦ç§»åŠ¨ä¸€æ ¼',
        'å³': 'å‘å³ç§»åŠ¨ä¸€æ ¼'
    }
    
    # æ„å»ºè½¬ç§»æ¦‚ç‡
    transition_probs = {}
    
    def get_next_position(pos, action):
        """æ ¹æ®å½“å‰ä½ç½®å’ŒåŠ¨ä½œè®¡ç®—ä¸‹ä¸€ä¸ªä½ç½®"""
        i, j = eval(pos)
        if action == 'ä¸Š' and i > 0:
            return f"({i-1},{j})"
        elif action == 'ä¸‹' and i < 3:
            return f"({i+1},{j})"
        elif action == 'å·¦' and j > 0:
            return f"({i},{j-1})"
        elif action == 'å³' and j < 3:
            return f"({i},{j+1})"
        else:
            return pos  # æ’å¢™ï¼Œä¿æŒåŸä½ç½®
    
    # ä¸ºæ¯ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹å®šä¹‰è½¬ç§»æ¦‚ç‡
    for state in states:
        for action in actions:
            next_state = get_next_position(state, action)
            
            # ç¡®å®šæ€§è½¬ç§»ï¼ˆ90%æ¦‚ç‡åˆ°è¾¾é¢„æœŸä½ç½®ï¼Œ10%æ¦‚ç‡ä¿æŒåŸä½ç½®ï¼‰
            transition_probs[(state, action, next_state)] = 0.9
            if next_state != state:
                transition_probs[(state, action, state)] = 0.1
            else:
                transition_probs[(state, action, state)] = 1.0
    
    # å®šä¹‰å¥–åŠ±å‡½æ•°
    rewards = {}
    for state in states:
        for action in actions:
            if state == '(0,3)':  # å®è—
                rewards[(state, action)] = 10.0
            elif state == '(1,3)':  # é™·é˜±
                rewards[(state, action)] = -10.0
            elif state == '(3,3)':  # ç»ˆç‚¹
                rewards[(state, action)] = 5.0
            else:
                rewards[(state, action)] = -0.1  # æ¯æ­¥å°æƒ©ç½šï¼Œé¼“åŠ±å¿«é€Ÿåˆ°è¾¾ç›®æ ‡
    
    # æŠ˜æ‰£å› å­
    gamma = 0.9
    
    print("ç½‘æ ¼ä¸–ç•Œè®¾ç½®:")
    print("  â€¢ 4x4ç½‘æ ¼ï¼Œ16ä¸ªçŠ¶æ€")
    print("  â€¢ 4ä¸ªåŠ¨ä½œï¼šä¸Šã€ä¸‹ã€å·¦ã€å³")
    print("  â€¢ ç‰¹æ®Šä½ç½®ï¼š")
    print("    - (0,3): å®è— (+10å¥–åŠ±)")
    print("    - (1,3): é™·é˜± (-10å¥–åŠ±)")
    print("    - (3,3): ç»ˆç‚¹ (+5å¥–åŠ±)")
    print("  â€¢ æ¯æ­¥ç§»åŠ¨ -0.1 å¥–åŠ±ï¼ˆé¼“åŠ±æ•ˆç‡ï¼‰")
    print("  â€¢ 90%æ¦‚ç‡æˆåŠŸç§»åŠ¨ï¼Œ10%æ¦‚ç‡åŸåœ°ä¸åŠ¨")
    
    # åˆ›å»ºMDP
    mdp = MarkovDecisionProcess(states, actions, transition_probs, rewards, gamma,
                               state_descriptions, action_descriptions)
    
    return mdp


def demonstrate_mdp_concepts():
    """æ¼”ç¤ºé©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹çš„æ ¸å¿ƒæ¦‚å¿µ"""
    print("\n" + "ğŸ¯" * 40)
    print("é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹ (MDP) æ ¸å¿ƒæ¦‚å¿µæ¼”ç¤º")
    print("ğŸ¯" * 40)
    
    # åˆ›å»ºç¤ºä¾‹MDP
    mdp = create_grid_world_mdp()
    
    # æ‰“å°è¯¦ç»†ä¿¡æ¯
    mdp.print_detailed_summary()
    
    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = MDPVisualizer(mdp)
    
    print("\n" + "="*60)
    print("ğŸ”„ ç¬¬ä¸€æ­¥ï¼šç­–ç•¥è¿­ä»£ç®—æ³•")
    print("="*60)
    
    # ç­–ç•¥è¿­ä»£
    optimal_policy_pi, optimal_value_pi = mdp.policy_iteration(verbose=True)
    
    print("\n" + "="*60)
    print("âš¡ ç¬¬äºŒæ­¥ï¼šä»·å€¼è¿­ä»£ç®—æ³•")
    print("="*60)
    
    # ä»·å€¼è¿­ä»£
    optimal_policy_vi, optimal_value_vi = mdp.value_iteration(verbose=True)
    
    print("\n" + "="*60)
    print("ğŸ“Š ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—Qå‡½æ•°")
    print("="*60)
    
    # è®¡ç®—Qå‡½æ•°
    Q_function = mdp.compute_q_function(optimal_policy_pi)
    print("åŠ¨ä½œä»·å€¼å‡½æ•° Q(s,a) è®¡ç®—å®Œæˆ")
    
    print("\n" + "="*60)
    print("ğŸ“ˆ ç¬¬å››æ­¥ï¼šç”Ÿæˆå¯è§†åŒ–åˆ†æ")
    print("="*60)
    
    print("æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    # 1. ç­–ç•¥å¯è§†åŒ–
    print("1. æœ€ä¼˜ç­–ç•¥å¯è§†åŒ–")
    visualizer.plot_policy_visualization(optimal_policy_pi, "ç­–ç•¥è¿­ä»£ - æœ€ä¼˜ç­–ç•¥")
    
    # 2. ä»·å€¼å‡½æ•°å¯è§†åŒ–
    print("2. çŠ¶æ€ä»·å€¼å‡½æ•°å¯è§†åŒ–")
    visualizer.plot_value_function(optimal_value_pi, "ç­–ç•¥è¿­ä»£ - æœ€ä¼˜ä»·å€¼å‡½æ•°")
    
    # 3. Qå‡½æ•°å¯è§†åŒ–
    print("3. åŠ¨ä½œä»·å€¼å‡½æ•°å¯è§†åŒ–")
    visualizer.plot_q_function(Q_function, "åŠ¨ä½œä»·å€¼å‡½æ•° Q(s,a)")
    
    # 4. æ”¶æ•›å†å²
    print("4. ç®—æ³•æ”¶æ•›å†å²")
    visualizer.plot_convergence_history()
    
    # 5. å›åˆæ¨¡æ‹Ÿ
    print("5. å›åˆæ¨¡æ‹Ÿåˆ†æ")
    returns, lengths = visualizer.plot_episode_analysis(
        optimal_policy_pi, '(0,0)', n_episodes=25, max_steps=20)
    
    # ç®—æ³•æ¯”è¾ƒ
    print("\n" + "="*80)
    print("ğŸ” ç®—æ³•æ¯”è¾ƒåˆ†æ")
    print("="*80)
    
    print("ç­–ç•¥è¿­ä»£ vs ä»·å€¼è¿­ä»£:")
    print(f"  ç­–ç•¥è¿­ä»£è½®æ•°: {len(mdp.policy_iteration_history)}")
    print(f"  ä»·å€¼è¿­ä»£è½®æ•°: {len(mdp.value_iteration_history)}")
    
    # æ¯”è¾ƒæœ€ä¼˜ä»·å€¼å‡½æ•°
    value_diff = np.max(np.abs(optimal_value_pi - optimal_value_vi))
    print(f"  ä»·å€¼å‡½æ•°å·®å¼‚: {value_diff:.8f}")
    
    # æ¯”è¾ƒæœ€ä¼˜ç­–ç•¥
    policy_diff = np.max(np.abs(optimal_policy_pi - optimal_policy_vi))
    print(f"  ç­–ç•¥å·®å¼‚: {policy_diff:.8f}")
    
    # æœ€ç»ˆæ€»ç»“
    print("\n" + "="*80)
    print("ğŸ“‹ é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹åˆ†ææ€»ç»“")
    print("="*80)
    
    print(f"\nğŸ¯ å…³é”®å‘ç°:")
    best_state_idx = np.argmax(optimal_value_pi)
    worst_state_idx = np.argmin(optimal_value_pi)
    print(f"  â€¢ æœ€ä¼˜çŠ¶æ€: {mdp.states[best_state_idx]} (V* = {optimal_value_pi[best_state_idx]:.3f})")
    print(f"  â€¢ æœ€å·®çŠ¶æ€: {mdp.states[worst_state_idx]} (V* = {optimal_value_vi[worst_state_idx]:.3f})")
    
    print(f"\nğŸ“Š æ¨¡æ‹ŸéªŒè¯:")
    print(f"  â€¢ å¹³å‡æŠ˜æ‰£å›æŠ¥: {np.mean(returns):.3f}")
    print(f"  â€¢ å›æŠ¥æ ‡å‡†å·®: {np.std(returns):.3f}")
    print(f"  â€¢ å¹³å‡å›åˆé•¿åº¦: {np.mean(lengths):.1f} æ­¥")
    
    print(f"\nğŸ’¡ æ•™å­¦è¦ç‚¹:")
    print(f"  1. MDPæ‰©å±•äº†MRPï¼Œå¢åŠ äº†åŠ¨ä½œé€‰æ‹©")
    print(f"  2. ç­–ç•¥å®šä¹‰äº†åœ¨æ¯ä¸ªçŠ¶æ€ä¸‹çš„è¡Œä¸º")
    print(f"  3. è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹æè¿°äº†æœ€ä¼˜ä»·å€¼å‡½æ•°")
    print(f"  4. ç­–ç•¥è¿­ä»£å’Œä»·å€¼è¿­ä»£éƒ½èƒ½æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥")
    print(f"  5. Qå‡½æ•°å¸®åŠ©ç†è§£çŠ¶æ€-åŠ¨ä½œå¯¹çš„ä»·å€¼")


if __name__ == "__main__":
    # è¿è¡Œå®Œæ•´çš„MDPæ¦‚å¿µæ¼”ç¤º
    demonstrate_mdp_concepts()