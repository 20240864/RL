"""
è´å°”æ›¼æœŸæœ›æ–¹ç¨‹ä¸è’™ç‰¹å¡æ´›æ–¹æ³• (Bellman Expectation Equation & Monte Carlo Methods) è¯¦ç»†å®ç°

æœ¬æ–‡ä»¶æä¾›äº†è´å°”æ›¼æœŸæœ›æ–¹ç¨‹å’Œè’™ç‰¹å¡æ´›æ–¹æ³•çš„å®Œæ•´å®ç°ï¼ŒåŒ…æ‹¬ï¼š
1. è´å°”æ›¼æœŸæœ›æ–¹ç¨‹çš„æ•°å­¦æ¨å¯¼å’Œå®ç°
2. è’™ç‰¹å¡æ´›é¢„æµ‹æ–¹æ³•ï¼ˆFirst-Visit MC, Every-Visit MCï¼‰
3. è’™ç‰¹å¡æ´›æ§åˆ¶æ–¹æ³•ï¼ˆMC Control with Îµ-greedyï¼‰
4. é‡è¦æ€§é‡‡æ ·æ–¹æ³•ï¼ˆImportance Samplingï¼‰
5. ä¸°å¯Œçš„å¯è§†åŒ–åŠŸèƒ½å’Œæ•™å­¦ç¤ºä¾‹

ä½œè€…ï¼šå¼ºåŒ–å­¦ä¹ æ•™å­¦ä»£ç 
ç‰ˆæœ¬ï¼š2.0
ç”¨é€”ï¼šæ·±å…¥ç†è§£è´å°”æ›¼æœŸæœ›æ–¹ç¨‹å’Œè’™ç‰¹å¡æ´›æ–¹æ³•çš„æ•°å­¦åŸç†å’Œå®é™…åº”ç”¨
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Tuple, Optional, Union, Callable
import warnings
from collections import defaultdict, deque
import random
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class BellmanExpectationEquation:
    """
    è´å°”æ›¼æœŸæœ›æ–¹ç¨‹è¯¦ç»†å®ç°ç±»
    
    è´å°”æ›¼æœŸæœ›æ–¹ç¨‹æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ•°å­¦åŸºç¡€ï¼š
    
    çŠ¶æ€ä»·å€¼å‡½æ•°çš„è´å°”æ›¼æœŸæœ›æ–¹ç¨‹ï¼š
    V^Ï€(s) = E_Ï€[R_{t+1} + Î³*V^Ï€(S_{t+1}) | S_t = s]
           = Î£_a Ï€(a|s) * Î£_{s',r} p(s',r|s,a) * [r + Î³*V^Ï€(s')]
    
    åŠ¨ä½œä»·å€¼å‡½æ•°çš„è´å°”æ›¼æœŸæœ›æ–¹ç¨‹ï¼š
    Q^Ï€(s,a) = E_Ï€[R_{t+1} + Î³*Q^Ï€(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]
             = Î£_{s',r} p(s',r|s,a) * [r + Î³ * Î£_{a'} Ï€(a'|s') * Q^Ï€(s',a')]
    
    æ ¸å¿ƒæ¦‚å¿µï¼š
    1. æœŸæœ›ï¼šå¯¹æ‰€æœ‰å¯èƒ½çš„æœªæ¥è½¨è¿¹æ±‚æœŸæœ›
    2. é€’å½’æ€§ï¼šå½“å‰ä»·å€¼ä¾èµ–äºæœªæ¥ä»·å€¼
    3. ç­–ç•¥ä¾èµ–ï¼šä»·å€¼å‡½æ•°ä¾èµ–äºæ‰€éµå¾ªçš„ç­–ç•¥
    4. é©¬å°”å¯å¤«æ€§ï¼šæœªæ¥åªä¾èµ–äºå½“å‰çŠ¶æ€
    """
    
    def __init__(self, states: List[str], actions: List[str], 
                 transition_probs: Dict[Tuple[str, str, str], float],
                 rewards: Dict[Tuple[str, str, str], float], 
                 gamma: float = 0.9):
        """
        åˆå§‹åŒ–è´å°”æ›¼æœŸæœ›æ–¹ç¨‹æ±‚è§£å™¨
        
        å‚æ•°:
            states: çŠ¶æ€ç©ºé—´
            actions: åŠ¨ä½œç©ºé—´
            transition_probs: è½¬ç§»æ¦‚ç‡ {(s, a, s'): P(s'|s,a)}
            rewards: å¥–åŠ±å‡½æ•° {(s, a, s'): R(s,a,s')}
            gamma: æŠ˜æ‰£å› å­
        """
        self.states = states
        self.actions = actions
        self.n_states = len(states)
        self.n_actions = len(actions)
        self.gamma = gamma
        
        # æ„å»ºç´¢å¼•æ˜ å°„
        self.state_to_idx = {s: i for i, s in enumerate(states)}
        self.action_to_idx = {a: i for i, a in enumerate(actions)}
        
        # æ„å»ºè½¬ç§»æ¦‚ç‡å’Œå¥–åŠ±çŸ©é˜µ
        self.P = np.zeros((self.n_states, self.n_actions, self.n_states))
        self.R = np.zeros((self.n_states, self.n_actions, self.n_states))
        
        for (s, a, s_next), prob in transition_probs.items():
            i, j, k = self.state_to_idx[s], self.action_to_idx[a], self.state_to_idx[s_next]
            self.P[i, j, k] = prob
        
        for (s, a, s_next), reward in rewards.items():
            i, j, k = self.state_to_idx[s], self.action_to_idx[a], self.state_to_idx[s_next]
            self.R[i, j, k] = reward
        
        print(f"âœ… è´å°”æ›¼æœŸæœ›æ–¹ç¨‹æ±‚è§£å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   çŠ¶æ€æ•°é‡: {self.n_states}")
        print(f"   åŠ¨ä½œæ•°é‡: {self.n_actions}")
        print(f"   æŠ˜æ‰£å› å­: {self.gamma}")
    
    def solve_bellman_expectation_v(self, policy: np.ndarray, 
                                   method: str = 'iterative',
                                   max_iterations: int = 1000,
                                   tolerance: float = 1e-8) -> np.ndarray:
        """
        æ±‚è§£çŠ¶æ€ä»·å€¼å‡½æ•°çš„è´å°”æ›¼æœŸæœ›æ–¹ç¨‹
        
        V^Ï€(s) = Î£_a Ï€(a|s) * Î£_{s'} P(s'|s,a) * [R(s,a,s') + Î³*V^Ï€(s')]
        
        å‚æ•°:
            policy: ç­–ç•¥çŸ©é˜µ Ï€(a|s)
            method: æ±‚è§£æ–¹æ³• ('iterative' æˆ– 'analytical')
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            tolerance: æ”¶æ•›å®¹å¿åº¦
            
        è¿”å›:
            çŠ¶æ€ä»·å€¼å‡½æ•° V^Ï€
        """
        print(f"\nğŸ§® æ±‚è§£çŠ¶æ€ä»·å€¼å‡½æ•°è´å°”æ›¼æœŸæœ›æ–¹ç¨‹ - {method}æ–¹æ³•")
        print("=" * 60)
        
        if method == 'analytical':
            return self._solve_v_analytical(policy)
        else:
            return self._solve_v_iterative(policy, max_iterations, tolerance)
    
    def _solve_v_analytical(self, policy: np.ndarray) -> np.ndarray:
        """è§£ææ±‚è§£çŠ¶æ€ä»·å€¼å‡½æ•°"""
        print("ä½¿ç”¨è§£ææ–¹æ³•æ±‚è§£...")
        
        # æ„å»ºç³»æ•°çŸ©é˜µ A å’Œå¸¸æ•°å‘é‡ b
        # (I - Î³ * P^Ï€) * V = R^Ï€
        A = np.eye(self.n_states)
        b = np.zeros(self.n_states)
        
        for s in range(self.n_states):
            for a in range(self.n_actions):
                pi_sa = policy[s, a]
                for s_next in range(self.n_states):
                    p_sas = self.P[s, a, s_next]
                    r_sas = self.R[s, a, s_next]
                    
                    # æ„å»ºç³»æ•°çŸ©é˜µ
                    A[s, s_next] -= self.gamma * pi_sa * p_sas
                    
                    # æ„å»ºå¸¸æ•°å‘é‡
                    b[s] += pi_sa * p_sas * r_sas
        
        print(f"ç³»æ•°çŸ©é˜µ A çš„æ¡ä»¶æ•°: {np.linalg.cond(A):.2e}")
        
        try:
            V = np.linalg.solve(A, b)
            print("âœ… è§£ææ±‚è§£æˆåŠŸ")
            return V
        except np.linalg.LinAlgError:
            print("âŒ è§£ææ±‚è§£å¤±è´¥ï¼ŒçŸ©é˜µå¥‡å¼‚")
            return self._solve_v_iterative(policy, 1000, 1e-8)
    
    def _solve_v_iterative(self, policy: np.ndarray, max_iterations: int, tolerance: float) -> np.ndarray:
        """è¿­ä»£æ±‚è§£çŠ¶æ€ä»·å€¼å‡½æ•°"""
        print("ä½¿ç”¨è¿­ä»£æ–¹æ³•æ±‚è§£...")
        
        V = np.zeros(self.n_states)
        
        for iteration in range(max_iterations):
            V_old = V.copy()
            
            for s in range(self.n_states):
                v_new = 0
                for a in range(self.n_actions):
                    pi_sa = policy[s, a]
                    for s_next in range(self.n_states):
                        p_sas = self.P[s, a, s_next]
                        r_sas = self.R[s, a, s_next]
                        v_new += pi_sa * p_sas * (r_sas + self.gamma * V_old[s_next])
                V[s] = v_new
            
            delta = np.max(np.abs(V - V_old))
            
            if iteration < 10 or iteration % 100 == 0:
                print(f"   è¿­ä»£ {iteration:3d}: Î´ = {delta:.8f}")
            
            if delta < tolerance:
                print(f"âœ… åœ¨ç¬¬ {iteration} æ¬¡è¿­ä»£åæ”¶æ•›")
                break
        
        return V
    
    def solve_bellman_expectation_q(self, policy: np.ndarray, 
                                   max_iterations: int = 1000,
                                   tolerance: float = 1e-8) -> np.ndarray:
        """
        æ±‚è§£åŠ¨ä½œä»·å€¼å‡½æ•°çš„è´å°”æ›¼æœŸæœ›æ–¹ç¨‹
        
        Q^Ï€(s,a) = Î£_{s'} P(s'|s,a) * [R(s,a,s') + Î³ * Î£_{a'} Ï€(a'|s') * Q^Ï€(s',a')]
        
        å‚æ•°:
            policy: ç­–ç•¥çŸ©é˜µ
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            tolerance: æ”¶æ•›å®¹å¿åº¦
            
        è¿”å›:
            åŠ¨ä½œä»·å€¼å‡½æ•° Q^Ï€
        """
        print(f"\nğŸ¯ æ±‚è§£åŠ¨ä½œä»·å€¼å‡½æ•°è´å°”æ›¼æœŸæœ›æ–¹ç¨‹")
        print("=" * 60)
        
        Q = np.zeros((self.n_states, self.n_actions))
        
        for iteration in range(max_iterations):
            Q_old = Q.copy()
            
            for s in range(self.n_states):
                for a in range(self.n_actions):
                    q_new = 0
                    for s_next in range(self.n_states):
                        p_sas = self.P[s, a, s_next]
                        r_sas = self.R[s, a, s_next]
                        
                        # è®¡ç®— Î£_{a'} Ï€(a'|s') * Q^Ï€(s',a')
                        expected_q = np.sum(policy[s_next] * Q_old[s_next])
                        
                        q_new += p_sas * (r_sas + self.gamma * expected_q)
                    
                    Q[s, a] = q_new
            
            delta = np.max(np.abs(Q - Q_old))
            
            if iteration < 10 or iteration % 100 == 0:
                print(f"   è¿­ä»£ {iteration:3d}: Î´ = {delta:.8f}")
            
            if delta < tolerance:
                print(f"âœ… åœ¨ç¬¬ {iteration} æ¬¡è¿­ä»£åæ”¶æ•›")
                break
        
        return Q


class MonteCarloMethods:
    """
    è’™ç‰¹å¡æ´›æ–¹æ³•è¯¦ç»†å®ç°ç±»
    
    è’™ç‰¹å¡æ´›æ–¹æ³•æ˜¯ä¸€ç±»åŸºäºé‡‡æ ·çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼š
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    1. é€šè¿‡ä¸ç¯å¢ƒäº¤äº’ç”Ÿæˆå®Œæ•´çš„å›åˆåºåˆ—
    2. ä½¿ç”¨å®é™…å›æŠ¥æ¥ä¼°è®¡ä»·å€¼å‡½æ•°
    3. ä¸éœ€è¦ç¯å¢ƒçš„å®Œæ•´æ¨¡å‹
    4. é€‚ç”¨äºå›åˆæ€§ä»»åŠ¡
    
    ä¸»è¦æ–¹æ³•ï¼š
    1. First-Visit MC: åªä½¿ç”¨çŠ¶æ€åœ¨å›åˆä¸­çš„é¦–æ¬¡è®¿é—®
    2. Every-Visit MC: ä½¿ç”¨çŠ¶æ€åœ¨å›åˆä¸­çš„æ¯æ¬¡è®¿é—®
    3. MC Control: ç»“åˆç­–ç•¥æ”¹è¿›çš„è’™ç‰¹å¡æ´›æ§åˆ¶
    4. é‡è¦æ€§é‡‡æ ·: å¤„ç†off-policyå­¦ä¹ 
    """
    
    def __init__(self, states: List[str], actions: List[str], gamma: float = 0.9):
        """
        åˆå§‹åŒ–è’™ç‰¹å¡æ´›æ–¹æ³•
        
        å‚æ•°:
            states: çŠ¶æ€ç©ºé—´
            actions: åŠ¨ä½œç©ºé—´
            gamma: æŠ˜æ‰£å› å­
        """
        self.states = states
        self.actions = actions
        self.n_states = len(states)
        self.n_actions = len(actions)
        self.gamma = gamma
        
        self.state_to_idx = {s: i for i, s in enumerate(states)}
        self.action_to_idx = {a: i for i, a in enumerate(actions)}
        
        # å­˜å‚¨å­¦ä¹ å†å²
        self.learning_history = []
        self.episode_returns = []
        
        print(f"âœ… è’™ç‰¹å¡æ´›æ–¹æ³•åˆå§‹åŒ–å®Œæˆ")
        print(f"   çŠ¶æ€æ•°é‡: {self.n_states}")
        print(f"   åŠ¨ä½œæ•°é‡: {self.n_actions}")
        print(f"   æŠ˜æ‰£å› å­: {self.gamma}")
    
    def first_visit_mc_prediction(self, episodes: List[List[Tuple]], 
                                 verbose: bool = True) -> np.ndarray:
        """
        First-Visit è’™ç‰¹å¡æ´›é¢„æµ‹
        
        å¯¹äºæ¯ä¸ªå›åˆä¸­çŠ¶æ€çš„é¦–æ¬¡è®¿é—®ï¼Œä½¿ç”¨è¯¥è®¿é—®ä¹‹åçš„å›æŠ¥æ¥æ›´æ–°ä»·å€¼ä¼°è®¡
        
        å‚æ•°:
            episodes: å›åˆåˆ—è¡¨ï¼Œæ¯ä¸ªå›åˆæ˜¯ [(state, action, reward), ...] çš„åºåˆ—
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        è¿”å›:
            çŠ¶æ€ä»·å€¼å‡½æ•°ä¼°è®¡
        """
        if verbose:
            print(f"\nğŸ² First-Visit è’™ç‰¹å¡æ´›é¢„æµ‹")
            print("=" * 50)
            print(f"   å›åˆæ•°é‡: {len(episodes)}")
        
        # åˆå§‹åŒ–
        returns = defaultdict(list)  # å­˜å‚¨æ¯ä¸ªçŠ¶æ€çš„å›æŠ¥åˆ—è¡¨
        V = np.zeros(self.n_states)
        
        for episode_idx, episode in enumerate(episodes):
            # è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„å›æŠ¥
            G = 0
            episode_returns = []
            for t in reversed(range(len(episode))):
                state, action, reward = episode[t]
                G = reward + self.gamma * G
                episode_returns.append(G)
            episode_returns.reverse()
            
            # First-Visit: åªè€ƒè™‘çŠ¶æ€çš„é¦–æ¬¡è®¿é—®
            visited_states = set()
            for t, (state, action, reward) in enumerate(episode):
                if state not in visited_states:
                    visited_states.add(state)
                    returns[state].append(episode_returns[t])
            
            # æ›´æ–°ä»·å€¼å‡½æ•°ä¼°è®¡
            if episode_idx % 100 == 0 or episode_idx < 10:
                for state in self.states:
                    if state in returns:
                        V[self.state_to_idx[state]] = np.mean(returns[state])
                
                if verbose and (episode_idx % 100 == 0):
                    print(f"   å›åˆ {episode_idx:4d}: å·²è®¿é—®çŠ¶æ€æ•° = {len(returns)}")
        
        # æœ€ç»ˆæ›´æ–°
        for state in self.states:
            if state in returns:
                V[self.state_to_idx[state]] = np.mean(returns[state])
        
        if verbose:
            print(f"âœ… First-Visit MCé¢„æµ‹å®Œæˆ")
            print(f"   è®¿é—®è¿‡çš„çŠ¶æ€æ•°: {len(returns)}")
            for state in self.states:
                if state in returns:
                    visits = len(returns[state])
                    value = V[self.state_to_idx[state]]
                    print(f"   V({state}) = {value:.4f} (åŸºäº {visits} æ¬¡è®¿é—®)")
        
        return V
    
    def every_visit_mc_prediction(self, episodes: List[List[Tuple]], 
                                 verbose: bool = True) -> np.ndarray:
        """
        Every-Visit è’™ç‰¹å¡æ´›é¢„æµ‹
        
        å¯¹äºæ¯ä¸ªå›åˆä¸­çŠ¶æ€çš„æ¯æ¬¡è®¿é—®ï¼Œéƒ½ä½¿ç”¨è¯¥è®¿é—®ä¹‹åçš„å›æŠ¥æ¥æ›´æ–°ä»·å€¼ä¼°è®¡
        
        å‚æ•°:
            episodes: å›åˆåˆ—è¡¨
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        è¿”å›:
            çŠ¶æ€ä»·å€¼å‡½æ•°ä¼°è®¡
        """
        if verbose:
            print(f"\nğŸ¯ Every-Visit è’™ç‰¹å¡æ´›é¢„æµ‹")
            print("=" * 50)
            print(f"   å›åˆæ•°é‡: {len(episodes)}")
        
        # åˆå§‹åŒ–
        returns = defaultdict(list)
        V = np.zeros(self.n_states)
        
        for episode_idx, episode in enumerate(episodes):
            # è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„å›æŠ¥
            G = 0
            episode_returns = []
            for t in reversed(range(len(episode))):
                state, action, reward = episode[t]
                G = reward + self.gamma * G
                episode_returns.append(G)
            episode_returns.reverse()
            
            # Every-Visit: è€ƒè™‘çŠ¶æ€çš„æ¯æ¬¡è®¿é—®
            for t, (state, action, reward) in enumerate(episode):
                returns[state].append(episode_returns[t])
            
            # æ›´æ–°ä»·å€¼å‡½æ•°ä¼°è®¡
            if episode_idx % 100 == 0 or episode_idx < 10:
                for state in self.states:
                    if state in returns:
                        V[self.state_to_idx[state]] = np.mean(returns[state])
                
                if verbose and (episode_idx % 100 == 0):
                    total_visits = sum(len(returns[s]) for s in returns)
                    print(f"   å›åˆ {episode_idx:4d}: æ€»è®¿é—®æ¬¡æ•° = {total_visits}")
        
        # æœ€ç»ˆæ›´æ–°
        for state in self.states:
            if state in returns:
                V[self.state_to_idx[state]] = np.mean(returns[state])
        
        if verbose:
            print(f"âœ… Every-Visit MCé¢„æµ‹å®Œæˆ")
            total_visits = sum(len(returns[s]) for s in returns if s in returns)
            print(f"   æ€»è®¿é—®æ¬¡æ•°: {total_visits}")
            for state in self.states:
                if state in returns:
                    visits = len(returns[state])
                    value = V[self.state_to_idx[state]]
                    print(f"   V({state}) = {value:.4f} (åŸºäº {visits} æ¬¡è®¿é—®)")
        
        return V
    
    def mc_control_epsilon_greedy(self, environment_simulator: Callable,
                                 num_episodes: int = 1000,
                                 epsilon: float = 0.1,
                                 epsilon_decay: float = 0.995,
                                 verbose: bool = True) -> Tuple[np.ndarray, np.ndarray]:
        """
        è’™ç‰¹å¡æ´›æ§åˆ¶ with Îµ-è´ªå©ªç­–ç•¥
        
        äº¤æ›¿è¿›è¡Œç­–ç•¥è¯„ä¼°å’Œç­–ç•¥æ”¹è¿›ï¼š
        1. ä½¿ç”¨å½“å‰ç­–ç•¥ç”Ÿæˆå›åˆ
        2. ä½¿ç”¨MCæ–¹æ³•æ›´æ–°Qå‡½æ•°
        3. ä½¿ç”¨Îµ-è´ªå©ªç­–ç•¥æ”¹è¿›ç­–ç•¥
        
        å‚æ•°:
            environment_simulator: ç¯å¢ƒæ¨¡æ‹Ÿå™¨å‡½æ•°
            num_episodes: å›åˆæ•°
            epsilon: Îµ-è´ªå©ªå‚æ•°
            epsilon_decay: Îµè¡°å‡ç‡
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        è¿”å›:
            (æœ€ä¼˜ç­–ç•¥, Qå‡½æ•°)
        """
        if verbose:
            print(f"\nğŸ® è’™ç‰¹å¡æ´›æ§åˆ¶ (Îµ-è´ªå©ªç­–ç•¥)")
            print("=" * 50)
            print(f"   å›åˆæ•°: {num_episodes}")
            print(f"   åˆå§‹Îµ: {epsilon}")
            print(f"   Îµè¡°å‡ç‡: {epsilon_decay}")
        
        # åˆå§‹åŒ–
        Q = np.zeros((self.n_states, self.n_actions))
        returns = defaultdict(list)
        policy = np.ones((self.n_states, self.n_actions)) / self.n_actions
        
        episode_rewards = []
        epsilon_history = []
        
        for episode in range(num_episodes):
            # ç”Ÿæˆå›åˆ
            episode_data = environment_simulator(policy, epsilon)
            states, actions, rewards = episode_data
            
            # è®¡ç®—å›æŠ¥
            G = 0
            episode_returns = []
            for t in reversed(range(len(rewards))):
                G = rewards[t] + self.gamma * G
                episode_returns.append(G)
            episode_returns.reverse()
            
            # æ›´æ–°Qå‡½æ•° (First-Visit)
            visited_sa = set()
            for t in range(len(states)-1):  # æœ€åä¸€ä¸ªçŠ¶æ€æ²¡æœ‰åŠ¨ä½œ
                s, a = states[t], actions[t]
                sa_pair = (s, a)
                
                if sa_pair not in visited_sa:
                    visited_sa.add(sa_pair)
                    s_idx = self.state_to_idx[s]
                    a_idx = self.action_to_idx[a]
                    returns[sa_pair].append(episode_returns[t])
                    Q[s_idx, a_idx] = np.mean(returns[sa_pair])
            
            # ç­–ç•¥æ”¹è¿› (Îµ-è´ªå©ª)
            for s_idx in range(self.n_states):
                best_action = np.argmax(Q[s_idx])
                for a_idx in range(self.n_actions):
                    if a_idx == best_action:
                        policy[s_idx, a_idx] = 1 - epsilon + epsilon / self.n_actions
                    else:
                        policy[s_idx, a_idx] = epsilon / self.n_actions
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            episode_reward = sum(rewards)
            episode_rewards.append(episode_reward)
            epsilon_history.append(epsilon)
            
            # Îµè¡°å‡
            epsilon = max(0.01, epsilon * epsilon_decay)
            
            if verbose and (episode % 100 == 0):
                avg_reward = np.mean(episode_rewards[-100:]) if episode >= 100 else np.mean(episode_rewards)
                print(f"   å›åˆ {episode:4d}: å¹³å‡å¥–åŠ± = {avg_reward:6.2f}, Îµ = {epsilon:.4f}")
        
        self.episode_returns = episode_rewards
        self.learning_history = epsilon_history
        
        if verbose:
            print(f"âœ… è’™ç‰¹å¡æ´›æ§åˆ¶å®Œæˆ")
            print(f"   æœ€ç»ˆÎµ: {epsilon:.4f}")
            print(f"   æœ€å100å›åˆå¹³å‡å¥–åŠ±: {np.mean(episode_rewards[-100:]):.2f}")
        
        return policy, Q
    
    def importance_sampling_prediction(self, episodes: List[List[Tuple]], 
                                     behavior_policy: np.ndarray,
                                     target_policy: np.ndarray,
                                     method: str = 'weighted',
                                     verbose: bool = True) -> np.ndarray:
        """
        é‡è¦æ€§é‡‡æ ·è’™ç‰¹å¡æ´›é¢„æµ‹
        
        ä½¿ç”¨è¡Œä¸ºç­–ç•¥ç”Ÿæˆçš„æ•°æ®æ¥è¯„ä¼°ç›®æ ‡ç­–ç•¥çš„ä»·å€¼å‡½æ•°
        
        å‚æ•°:
            episodes: å›åˆåˆ—è¡¨ [(state, action, reward), ...]
            behavior_policy: è¡Œä¸ºç­–ç•¥ (ç”Ÿæˆæ•°æ®çš„ç­–ç•¥)
            target_policy: ç›®æ ‡ç­–ç•¥ (è¦è¯„ä¼°çš„ç­–ç•¥)
            method: 'ordinary' æˆ– 'weighted' é‡è¦æ€§é‡‡æ ·
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        è¿”å›:
            ç›®æ ‡ç­–ç•¥çš„çŠ¶æ€ä»·å€¼å‡½æ•°ä¼°è®¡
        """
        if verbose:
            print(f"\nğŸ­ é‡è¦æ€§é‡‡æ ·è’™ç‰¹å¡æ´›é¢„æµ‹ ({method})")
            print("=" * 50)
            print(f"   å›åˆæ•°é‡: {len(episodes)}")
        
        if method == 'ordinary':
            return self._ordinary_importance_sampling(episodes, behavior_policy, target_policy, verbose)
        else:
            return self._weighted_importance_sampling(episodes, behavior_policy, target_policy, verbose)
    
    def _ordinary_importance_sampling(self, episodes, behavior_policy, target_policy, verbose):
        """æ™®é€šé‡è¦æ€§é‡‡æ ·"""
        returns = defaultdict(list)
        importance_ratios = defaultdict(list)
        
        for episode in episodes:
            # è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡
            rho = 1.0
            for state, action, reward in episode:
                s_idx = self.state_to_idx[state]
                a_idx = self.action_to_idx[action]
                
                pi_target = target_policy[s_idx, a_idx]
                pi_behavior = behavior_policy[s_idx, a_idx]
                
                if pi_behavior > 0:
                    rho *= pi_target / pi_behavior
                else:
                    rho = 0
                    break
            
            # è®¡ç®—å›æŠ¥
            G = 0
            episode_returns = []
            for t in reversed(range(len(episode))):
                state, action, reward = episode[t]
                G = reward + self.gamma * G
                episode_returns.append(G)
            episode_returns.reverse()
            
            # æ›´æ–°ä¼°è®¡ (First-Visit)
            visited_states = set()
            for t, (state, action, reward) in enumerate(episode):
                if state not in visited_states:
                    visited_states.add(state)
                    returns[state].append(rho * episode_returns[t])
                    importance_ratios[state].append(rho)
        
        # è®¡ç®—ä»·å€¼å‡½æ•°
        V = np.zeros(self.n_states)
        for state in self.states:
            if state in returns and len(returns[state]) > 0:
                V[self.state_to_idx[state]] = np.mean(returns[state])
        
        if verbose:
            print(f"âœ… æ™®é€šé‡è¦æ€§é‡‡æ ·å®Œæˆ")
            for state in self.states:
                if state in returns:
                    avg_ratio = np.mean(importance_ratios[state])
                    print(f"   V({state}) = {V[self.state_to_idx[state]]:.4f}, å¹³å‡é‡è¦æ€§æ¯”ç‡ = {avg_ratio:.4f}")
        
        return V
    
    def _weighted_importance_sampling(self, episodes, behavior_policy, target_policy, verbose):
        """åŠ æƒé‡è¦æ€§é‡‡æ ·"""
        weighted_returns = defaultdict(float)
        importance_weights = defaultdict(float)
        
        for episode in episodes:
            # è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡
            rho = 1.0
            for state, action, reward in episode:
                s_idx = self.state_to_idx[state]
                a_idx = self.action_to_idx[action]
                
                pi_target = target_policy[s_idx, a_idx]
                pi_behavior = behavior_policy[s_idx, a_idx]
                
                if pi_behavior > 0:
                    rho *= pi_target / pi_behavior
                else:
                    rho = 0
                    break
            
            # è®¡ç®—å›æŠ¥
            G = 0
            episode_returns = []
            for t in reversed(range(len(episode))):
                state, action, reward = episode[t]
                G = reward + self.gamma * G
                episode_returns.append(G)
            episode_returns.reverse()
            
            # æ›´æ–°åŠ æƒä¼°è®¡ (First-Visit)
            visited_states = set()
            for t, (state, action, reward) in enumerate(episode):
                if state not in visited_states:
                    visited_states.add(state)
                    weighted_returns[state] += rho * episode_returns[t]
                    importance_weights[state] += rho
        
        # è®¡ç®—ä»·å€¼å‡½æ•°
        V = np.zeros(self.n_states)
        for state in self.states:
            if state in weighted_returns and importance_weights[state] > 0:
                V[self.state_to_idx[state]] = weighted_returns[state] / importance_weights[state]
        
        if verbose:
            print(f"âœ… åŠ æƒé‡è¦æ€§é‡‡æ ·å®Œæˆ")
            for state in self.states:
                if state in weighted_returns:
                    weight = importance_weights[state]
                    print(f"   V({state}) = {V[self.state_to_idx[state]]:.4f}, æ€»æƒé‡ = {weight:.4f}")
        
        return V


class BellmanMonteCarloVisualizer:
    """è´å°”æ›¼æœŸæœ›æ–¹ç¨‹ä¸è’™ç‰¹å¡æ´›æ–¹æ³•å¯è§†åŒ–ç±»"""
    
    def __init__(self, bellman_solver: BellmanExpectationEquation, 
                 mc_methods: MonteCarloMethods):
        """
        åˆå§‹åŒ–å¯è§†åŒ–å™¨
        
        å‚æ•°:
            bellman_solver: è´å°”æ›¼æ–¹ç¨‹æ±‚è§£å™¨
            mc_methods: è’™ç‰¹å¡æ´›æ–¹æ³•å®ä¾‹
        """
        self.bellman = bellman_solver
        self.mc = mc_methods
        plt.style.use('default')
        # é‡æ–°è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œç¡®ä¿åœ¨æ ·å¼è®¾ç½®åç”Ÿæ•ˆ
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
    
    def plot_bellman_equation_breakdown(self, policy: np.ndarray, 
                                      figsize: Tuple[int, int] = (15, 10)):
        """
        ç»˜åˆ¶è´å°”æ›¼æœŸæœ›æ–¹ç¨‹çš„åˆ†è§£å›¾
        
        å‚æ•°:
            policy: ç­–ç•¥çŸ©é˜µ
            figsize: å›¾å½¢å¤§å°
        """
        fig = plt.figure(figsize=figsize)
        gs = fig.add_gridspec(2, 3, height_ratios=[1, 1])
        
        # 1. ç­–ç•¥å¯è§†åŒ–
        ax1 = fig.add_subplot(gs[0, 0])
        im1 = ax1.imshow(policy, cmap='Blues', aspect='auto')
        ax1.set_title('ç­–ç•¥ Ï€(a|s)')
        ax1.set_xlabel('åŠ¨ä½œ')
        ax1.set_ylabel('çŠ¶æ€')
        ax1.set_xticks(range(len(self.bellman.actions)))
        ax1.set_xticklabels(self.bellman.actions)
        ax1.set_yticks(range(len(self.bellman.states)))
        ax1.set_yticklabels(self.bellman.states)
        plt.colorbar(im1, ax=ax1)
        
        # 2. å³æ—¶å¥–åŠ±
        ax2 = fig.add_subplot(gs[0, 1])
        immediate_rewards = np.zeros((self.bellman.n_states, self.bellman.n_actions))
        for s in range(self.bellman.n_states):
            for a in range(self.bellman.n_actions):
                immediate_rewards[s, a] = np.sum(self.bellman.P[s, a, :] * self.bellman.R[s, a, :])
        
        im2 = ax2.imshow(immediate_rewards, cmap='RdYlGn', aspect='auto')
        ax2.set_title('å³æ—¶å¥–åŠ± E[R|s,a]')
        ax2.set_xlabel('åŠ¨ä½œ')
        ax2.set_ylabel('çŠ¶æ€')
        ax2.set_xticks(range(len(self.bellman.actions)))
        ax2.set_xticklabels(self.bellman.actions)
        ax2.set_yticks(range(len(self.bellman.states)))
        ax2.set_yticklabels(self.bellman.states)
        plt.colorbar(im2, ax=ax2)
        
        # 3. çŠ¶æ€ä»·å€¼å‡½æ•°
        ax3 = fig.add_subplot(gs[0, 2])
        V = self.bellman.solve_bellman_expectation_v(policy, method='iterative')
        bars = ax3.bar(self.bellman.states, V, color='skyblue', alpha=0.7)
        ax3.set_title('çŠ¶æ€ä»·å€¼å‡½æ•° V^Ï€(s)')
        ax3.set_xlabel('çŠ¶æ€')
        ax3.set_ylabel('ä»·å€¼')
        ax3.tick_params(axis='x', rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for bar, value in zip(bars, V):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                    f'{value:.2f}', ha='center', va='bottom')
        
        # 4. è´å°”æ›¼æ–¹ç¨‹åˆ†è§£
        ax4 = fig.add_subplot(gs[1, :])
        ax4.axis('off')
        
        # æ˜¾ç¤ºè´å°”æ›¼æ–¹ç¨‹çš„æ•°å­¦å½¢å¼å’Œæ•°å€¼è®¡ç®—
        equation_text = """
        è´å°”æ›¼æœŸæœ›æ–¹ç¨‹åˆ†è§£ï¼š
        
        V^Ï€(s) = Î£_a Ï€(a|s) Ã— Î£_{s'} P(s'|s,a) Ã— [R(s,a,s') + Î³ Ã— V^Ï€(s')]
                 â†‘           â†‘                      â†‘              â†‘
              ç­–ç•¥æ¦‚ç‡    è½¬ç§»æ¦‚ç‡              å³æ—¶å¥–åŠ±      æŠ˜æ‰£æœªæ¥ä»·å€¼
        
        æ•°å€¼ç¤ºä¾‹ (ä»¥ç¬¬ä¸€ä¸ªçŠ¶æ€ä¸ºä¾‹):
        """
        
        # è®¡ç®—ç¬¬ä¸€ä¸ªçŠ¶æ€çš„è´å°”æ›¼æ–¹ç¨‹åˆ†è§£
        s = 0
        breakdown_text = f"\n        V^Ï€({self.bellman.states[s]}) = "
        total_value = 0
        
        for a in range(self.bellman.n_actions):
            pi_sa = policy[s, a]
            if pi_sa > 0.001:  # åªæ˜¾ç¤ºæœ‰æ„ä¹‰çš„é¡¹
                action_value = 0
                for s_next in range(self.bellman.n_states):
                    p_sas = self.bellman.P[s, a, s_next]
                    r_sas = self.bellman.R[s, a, s_next]
                    if p_sas > 0:
                        action_value += p_sas * (r_sas + self.bellman.gamma * V[s_next])
                
                term_value = pi_sa * action_value
                total_value += term_value
                breakdown_text += f"\n                     + {pi_sa:.3f} Ã— {action_value:.3f} ({self.bellman.actions[a]})"
        
        breakdown_text += f"\n                   = {total_value:.3f}"
        
        equation_text += breakdown_text
        
        ax4.text(0.05, 0.95, equation_text, transform=ax4.transAxes, fontsize=11,
                verticalalignment='top', fontfamily='sans-serif',
                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.8))
        
        plt.suptitle('è´å°”æ›¼æœŸæœ›æ–¹ç¨‹è¯¦ç»†åˆ†è§£', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
    
    def plot_mc_convergence_analysis(self, true_values: np.ndarray,
                                   mc_first_visit: np.ndarray,
                                   mc_every_visit: np.ndarray,
                                   figsize: Tuple[int, int] = (15, 8)):
        """
        ç»˜åˆ¶è’™ç‰¹å¡æ´›æ–¹æ³•æ”¶æ•›åˆ†æ
        
        å‚æ•°:
            true_values: çœŸå®ä»·å€¼å‡½æ•°
            mc_first_visit: First-Visit MCä¼°è®¡
            mc_every_visit: Every-Visit MCä¼°è®¡
            figsize: å›¾å½¢å¤§å°
        """
        fig, axes = plt.subplots(2, 2, figsize=figsize)
        
        # 1. ä»·å€¼å‡½æ•°æ¯”è¾ƒ
        x = np.arange(len(self.mc.states))
        width = 0.25
        
        axes[0, 0].bar(x - width, true_values, width, label='çœŸå®å€¼', alpha=0.8, color='green')
        axes[0, 0].bar(x, mc_first_visit, width, label='First-Visit MC', alpha=0.8, color='blue')
        axes[0, 0].bar(x + width, mc_every_visit, width, label='Every-Visit MC', alpha=0.8, color='red')
        
        axes[0, 0].set_xlabel('çŠ¶æ€')
        axes[0, 0].set_ylabel('ä»·å€¼')
        axes[0, 0].set_title('ä»·å€¼å‡½æ•°ä¼°è®¡æ¯”è¾ƒ')
        axes[0, 0].set_xticks(x)
        axes[0, 0].set_xticklabels(self.mc.states, rotation=45)
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. è¯¯å·®åˆ†æ
        error_first = np.abs(mc_first_visit - true_values)
        error_every = np.abs(mc_every_visit - true_values)
        
        axes[0, 1].bar(x - width/2, error_first, width, label='First-Visit è¯¯å·®', alpha=0.8, color='blue')
        axes[0, 1].bar(x + width/2, error_every, width, label='Every-Visit è¯¯å·®', alpha=0.8, color='red')
        
        axes[0, 1].set_xlabel('çŠ¶æ€')
        axes[0, 1].set_ylabel('ç»å¯¹è¯¯å·®')
        axes[0, 1].set_title('ä¼°è®¡è¯¯å·®æ¯”è¾ƒ')
        axes[0, 1].set_xticks(x)
        axes[0, 1].set_xticklabels(self.mc.states, rotation=45)
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. å­¦ä¹ æ›²çº¿ (å¦‚æœæœ‰å†å²æ•°æ®)
        if hasattr(self.mc, 'episode_returns') and self.mc.episode_returns:
            episodes = range(len(self.mc.episode_returns))
            returns = self.mc.episode_returns
            
            # è®¡ç®—ç§»åŠ¨å¹³å‡
            window_size = min(50, len(returns) // 10)
            if window_size > 1:
                moving_avg = []
                for i in range(len(returns)):
                    start = max(0, i - window_size + 1)
                    moving_avg.append(np.mean(returns[start:i+1]))
                
                axes[1, 0].plot(episodes, returns, alpha=0.3, color='gray', label='å›åˆå¥–åŠ±')
                axes[1, 0].plot(episodes, moving_avg, color='blue', linewidth=2, label=f'{window_size}å›åˆç§»åŠ¨å¹³å‡')
            else:
                axes[1, 0].plot(episodes, returns, color='blue', label='å›åˆå¥–åŠ±')
            
            axes[1, 0].set_xlabel('å›åˆ')
            axes[1, 0].set_ylabel('å›åˆå¥–åŠ±')
            axes[1, 0].set_title('å­¦ä¹ æ›²çº¿')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
        else:
            axes[1, 0].text(0.5, 0.5, 'æ— å­¦ä¹ å†å²æ•°æ®', ha='center', va='center', 
                           transform=axes[1, 0].transAxes, fontsize=12)
            axes[1, 0].set_title('å­¦ä¹ æ›²çº¿')
        
        # 4. ç»Ÿè®¡æ‘˜è¦
        axes[1, 1].axis('off')
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        mse_first = np.mean((mc_first_visit - true_values) ** 2)
        mse_every = np.mean((mc_every_visit - true_values) ** 2)
        mae_first = np.mean(np.abs(mc_first_visit - true_values))
        mae_every = np.mean(np.abs(mc_every_visit - true_values))
        
        stats_text = f"""
        è’™ç‰¹å¡æ´›æ–¹æ³•æ€§èƒ½æ¯”è¾ƒ
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        First-Visit MC:
        â€¢ å‡æ–¹è¯¯å·® (MSE): {mse_first:.6f}
        â€¢ å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae_first:.6f}
        â€¢ æœ€å¤§è¯¯å·®: {np.max(error_first):.6f}
        
        Every-Visit MC:
        â€¢ å‡æ–¹è¯¯å·® (MSE): {mse_every:.6f}
        â€¢ å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae_every:.6f}
        â€¢ æœ€å¤§è¯¯å·®: {np.max(error_every):.6f}
        
        æ–¹æ³•æ¯”è¾ƒ:
        â€¢ æ›´ä¼˜æ–¹æ³•: {"First-Visit" if mse_first < mse_every else "Every-Visit"}
        â€¢ MSEæ”¹è¿›: {abs(mse_first - mse_every):.6f}
        """
        
        axes[1, 1].text(0.05, 0.95, stats_text, transform=axes[1, 1].transAxes, 
                        fontsize=11, verticalalignment='top', fontfamily='sans-serif',
                        bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))
        
        plt.suptitle('è’™ç‰¹å¡æ´›æ–¹æ³•æ”¶æ•›åˆ†æ', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
    
    def plot_importance_sampling_comparison(self, ordinary_is: np.ndarray,
                                          weighted_is: np.ndarray,
                                          true_values: np.ndarray,
                                          figsize: Tuple[int, int] = (12, 8)):
        """
        ç»˜åˆ¶é‡è¦æ€§é‡‡æ ·æ–¹æ³•æ¯”è¾ƒ
        
        å‚æ•°:
            ordinary_is: æ™®é€šé‡è¦æ€§é‡‡æ ·ç»“æœ
            weighted_is: åŠ æƒé‡è¦æ€§é‡‡æ ·ç»“æœ
            true_values: çœŸå®ä»·å€¼å‡½æ•°
            figsize: å›¾å½¢å¤§å°
        """
        fig, axes = plt.subplots(2, 2, figsize=figsize)
        
        # 1. ä»·å€¼å‡½æ•°æ¯”è¾ƒ
        x = np.arange(len(self.mc.states))
        width = 0.2
        
        axes[0, 0].bar(x - width, true_values, width, label='çœŸå®å€¼', alpha=0.8, color='green')
        axes[0, 0].bar(x, ordinary_is, width, label='æ™®é€šé‡è¦æ€§é‡‡æ ·', alpha=0.8, color='blue')
        axes[0, 0].bar(x + width, weighted_is, width, label='åŠ æƒé‡è¦æ€§é‡‡æ ·', alpha=0.8, color='red')
        
        axes[0, 0].set_xlabel('çŠ¶æ€')
        axes[0, 0].set_ylabel('ä»·å€¼')
        axes[0, 0].set_title('é‡è¦æ€§é‡‡æ ·æ–¹æ³•æ¯”è¾ƒ')
        axes[0, 0].set_xticks(x)
        axes[0, 0].set_xticklabels(self.mc.states, rotation=45)
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. è¯¯å·®åˆ†æ
        error_ordinary = np.abs(ordinary_is - true_values)
        error_weighted = np.abs(weighted_is - true_values)
        
        axes[0, 1].bar(x - width/2, error_ordinary, width, label='æ™®é€šISè¯¯å·®', alpha=0.8, color='blue')
        axes[0, 1].bar(x + width/2, error_weighted, width, label='åŠ æƒISè¯¯å·®', alpha=0.8, color='red')
        
        axes[0, 1].set_xlabel('çŠ¶æ€')
        axes[0, 1].set_ylabel('ç»å¯¹è¯¯å·®')
        axes[0, 1].set_title('é‡è¦æ€§é‡‡æ ·è¯¯å·®æ¯”è¾ƒ')
        axes[0, 1].set_xticks(x)
        axes[0, 1].set_xticklabels(self.mc.states, rotation=45)
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. æ–¹å·®åˆ†æ (æ¨¡æ‹Ÿ)
        # è¿™é‡Œæˆ‘ä»¬æ¨¡æ‹Ÿä¸åŒæ–¹æ³•çš„æ–¹å·®ç‰¹æ€§
        episodes_range = np.arange(10, 1001, 50)
        ordinary_variance = 1.0 / np.sqrt(episodes_range)  # æ™®é€šISæ–¹å·®è¾ƒé«˜
        weighted_variance = 0.5 / np.sqrt(episodes_range)  # åŠ æƒISæ–¹å·®è¾ƒä½
        
        axes[1, 0].plot(episodes_range, ordinary_variance, 'b-', label='æ™®é€šé‡è¦æ€§é‡‡æ ·', linewidth=2)
        axes[1, 0].plot(episodes_range, weighted_variance, 'r-', label='åŠ æƒé‡è¦æ€§é‡‡æ ·', linewidth=2)
        axes[1, 0].set_xlabel('å›åˆæ•°')
        axes[1, 0].set_ylabel('ä¼°è®¡æ–¹å·®')
        axes[1, 0].set_title('æ–¹å·®æ”¶æ•›ç‰¹æ€§')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        axes[1, 0].set_yscale('log')
        
        # 4. ç»Ÿè®¡æ‘˜è¦
        axes[1, 1].axis('off')
        
        mse_ordinary = np.mean((ordinary_is - true_values) ** 2)
        mse_weighted = np.mean((weighted_is - true_values) ** 2)
        
        stats_text = f"""
        é‡è¦æ€§é‡‡æ ·æ–¹æ³•æ€§èƒ½æ¯”è¾ƒ
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        æ™®é€šé‡è¦æ€§é‡‡æ ·:
        â€¢ å‡æ–¹è¯¯å·®: {mse_ordinary:.6f}
        â€¢ å¹³å‡ç»å¯¹è¯¯å·®: {np.mean(error_ordinary):.6f}
        â€¢ æœ€å¤§è¯¯å·®: {np.max(error_ordinary):.6f}
        â€¢ ç‰¹ç‚¹: æ— åä¼°è®¡ï¼Œä½†æ–¹å·®å¯èƒ½å¾ˆå¤§
        
        åŠ æƒé‡è¦æ€§é‡‡æ ·:
        â€¢ å‡æ–¹è¯¯å·®: {mse_weighted:.6f}
        â€¢ å¹³å‡ç»å¯¹è¯¯å·®: {np.mean(error_weighted):.6f}
        â€¢ æœ€å¤§è¯¯å·®: {np.max(error_weighted):.6f}
        â€¢ ç‰¹ç‚¹: æœ‰åä¼°è®¡ï¼Œä½†æ–¹å·®è¾ƒå°
        
        æ¨èæ–¹æ³•: {"åŠ æƒé‡è¦æ€§é‡‡æ ·" if mse_weighted < mse_ordinary else "æ™®é€šé‡è¦æ€§é‡‡æ ·"}
        """
        
        axes[1, 1].text(0.05, 0.95, stats_text, transform=axes[1, 1].transAxes, 
                        fontsize=10, verticalalignment='top', fontfamily='sans-serif',
                        bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow', alpha=0.8))
        
        plt.suptitle('é‡è¦æ€§é‡‡æ ·æ–¹æ³•æ¯”è¾ƒåˆ†æ', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()


def create_simple_mdp_example():
    """åˆ›å»ºç®€å•çš„MDPç¤ºä¾‹ç”¨äºæ¼”ç¤º"""
    print("\n" + "="*80)
    print("ğŸ¯ åˆ›å»ºç®€å•MDPç¤ºä¾‹ï¼šå­¦ç”Ÿå­¦ä¹ å†³ç­–è¿‡ç¨‹")
    print("="*80)
    
    states = ['å›°å€¦', 'æ¸…é†’', 'ä¸“æ³¨']
    actions = ['ä¼‘æ¯', 'å­¦ä¹ ']
    
    # è½¬ç§»æ¦‚ç‡ {(s, a, s'): P(s'|s,a)}
    transition_probs = {
        ('å›°å€¦', 'ä¼‘æ¯', 'å›°å€¦'): 0.3,
        ('å›°å€¦', 'ä¼‘æ¯', 'æ¸…é†’'): 0.7,
        ('å›°å€¦', 'å­¦ä¹ ', 'å›°å€¦'): 0.8,
        ('å›°å€¦', 'å­¦ä¹ ', 'æ¸…é†’'): 0.2,
        
        ('æ¸…é†’', 'ä¼‘æ¯', 'å›°å€¦'): 0.4,
        ('æ¸…é†’', 'ä¼‘æ¯', 'æ¸…é†’'): 0.6,
        ('æ¸…é†’', 'å­¦ä¹ ', 'æ¸…é†’'): 0.4,
        ('æ¸…é†’', 'å­¦ä¹ ', 'ä¸“æ³¨'): 0.6,
        
        ('ä¸“æ³¨', 'ä¼‘æ¯', 'æ¸…é†’'): 0.8,
        ('ä¸“æ³¨', 'ä¼‘æ¯', 'ä¸“æ³¨'): 0.2,
        ('ä¸“æ³¨', 'å­¦ä¹ ', 'ä¸“æ³¨'): 0.9,
        ('ä¸“æ³¨', 'å­¦ä¹ ', 'æ¸…é†’'): 0.1,
    }
    
    # å¥–åŠ±å‡½æ•° {(s, a, s'): R(s,a,s')}
    rewards = {
        ('å›°å€¦', 'ä¼‘æ¯', 'å›°å€¦'): -1,
        ('å›°å€¦', 'ä¼‘æ¯', 'æ¸…é†’'): 2,
        ('å›°å€¦', 'å­¦ä¹ ', 'å›°å€¦'): -5,
        ('å›°å€¦', 'å­¦ä¹ ', 'æ¸…é†’'): 1,
        
        ('æ¸…é†’', 'ä¼‘æ¯', 'å›°å€¦'): -2,
        ('æ¸…é†’', 'ä¼‘æ¯', 'æ¸…é†’'): 0,
        ('æ¸…é†’', 'å­¦ä¹ ', 'æ¸…é†’'): 3,
        ('æ¸…é†’', 'å­¦ä¹ ', 'ä¸“æ³¨'): 8,
        
        ('ä¸“æ³¨', 'ä¼‘æ¯', 'æ¸…é†’'): -3,
        ('ä¸“æ³¨', 'ä¼‘æ¯', 'ä¸“æ³¨'): 1,
        ('ä¸“æ³¨', 'å­¦ä¹ ', 'ä¸“æ³¨'): 10,
        ('ä¸“æ³¨', 'å­¦ä¹ ', 'æ¸…é†’'): 5,
    }
    
    return states, actions, transition_probs, rewards


def demonstrate_bellman_monte_carlo():
    """æ¼”ç¤ºè´å°”æ›¼æœŸæœ›æ–¹ç¨‹ä¸è’™ç‰¹å¡æ´›æ–¹æ³•"""
    print("\n" + "ğŸ§ " * 40)
    print("è´å°”æ›¼æœŸæœ›æ–¹ç¨‹ä¸è’™ç‰¹å¡æ´›æ–¹æ³•æ ¸å¿ƒæ¦‚å¿µæ¼”ç¤º")
    print("ğŸ§ " * 40)
    
    # åˆ›å»ºç¤ºä¾‹
    states, actions, transition_probs, rewards = create_simple_mdp_example()
    
    # åˆå§‹åŒ–æ±‚è§£å™¨
    bellman_solver = BellmanExpectationEquation(states, actions, transition_probs, rewards)
    mc_methods = MonteCarloMethods(states, actions)
    
    # åˆ›å»ºæµ‹è¯•ç­–ç•¥
    print("\nğŸ“‹ åˆ›å»ºæµ‹è¯•ç­–ç•¥")
    policy = np.array([
        [0.7, 0.3],  # å›°å€¦: 70%ä¼‘æ¯, 30%å­¦ä¹ 
        [0.4, 0.6],  # æ¸…é†’: 40%ä¼‘æ¯, 60%å­¦ä¹ 
        [0.2, 0.8],  # ä¸“æ³¨: 20%ä¼‘æ¯, 80%å­¦ä¹ 
    ])
    
    print("ç­–ç•¥è®¾ç½®:")
    for i, state in enumerate(states):
        print(f"  {state}: {policy[i, 0]:.1%}ä¼‘æ¯, {policy[i, 1]:.1%}å­¦ä¹ ")
    
    # 1. æ±‚è§£è´å°”æ›¼æœŸæœ›æ–¹ç¨‹
    print("\n" + "="*60)
    print("ğŸ§® ç¬¬ä¸€æ­¥ï¼šæ±‚è§£è´å°”æ›¼æœŸæœ›æ–¹ç¨‹")
    print("="*60)
    
    V_analytical = bellman_solver.solve_bellman_expectation_v(policy, method='analytical')
    V_iterative = bellman_solver.solve_bellman_expectation_v(policy, method='iterative')
    Q_function = bellman_solver.solve_bellman_expectation_q(policy)
    
    print(f"\nä»·å€¼å‡½æ•°æ¯”è¾ƒ:")
    print(f"{'çŠ¶æ€':<8} {'è§£æè§£':<10} {'è¿­ä»£è§£':<10} {'å·®å¼‚':<10}")
    print("-" * 40)
    for i, state in enumerate(states):
        diff = abs(V_analytical[i] - V_iterative[i])
        print(f"{state:<8} {V_analytical[i]:<10.4f} {V_iterative[i]:<10.4f} {diff:<10.6f}")
    
    # 2. ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    print("\n" + "="*60)
    print("ğŸ² ç¬¬äºŒæ­¥ï¼šç”Ÿæˆè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿæ•°æ®")
    print("="*60)
    
    def simple_environment_simulator(num_episodes=500, max_steps=20):
        """ç®€å•ç¯å¢ƒæ¨¡æ‹Ÿå™¨"""
        episodes = []
        
        for _ in range(num_episodes):
            episode = []
            state = np.random.choice(states)  # éšæœºèµ·å§‹çŠ¶æ€
            
            for _ in range(max_steps):
                # æ ¹æ®ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
                state_idx = bellman_solver.state_to_idx[state]
                action_probs = policy[state_idx]
                action_idx = np.random.choice(len(actions), p=action_probs)
                action = actions[action_idx]
                
                # çŠ¶æ€è½¬ç§»å’Œå¥–åŠ±
                next_state_probs = bellman_solver.P[state_idx, action_idx]
                next_state_idx = np.random.choice(len(states), p=next_state_probs)
                next_state = states[next_state_idx]
                
                reward = bellman_solver.R[state_idx, action_idx, next_state_idx]
                
                episode.append((state, action, reward))
                state = next_state
            
            episodes.append(episode)
        
        return episodes
    
    episodes = simple_environment_simulator(num_episodes=1000)
    print(f"ç”Ÿæˆäº† {len(episodes)} ä¸ªå›åˆçš„æ¨¡æ‹Ÿæ•°æ®")
    print(f"å¹³å‡å›åˆé•¿åº¦: {np.mean([len(ep) for ep in episodes]):.1f}")
    
    # 3. è’™ç‰¹å¡æ´›é¢„æµ‹
    print("\n" + "="*60)
    print("ğŸ¯ ç¬¬ä¸‰æ­¥ï¼šè’™ç‰¹å¡æ´›é¢„æµ‹æ–¹æ³•")
    print("="*60)
    
    V_first_visit = mc_methods.first_visit_mc_prediction(episodes)
    V_every_visit = mc_methods.every_visit_mc_prediction(episodes)
    
    # 4. é‡è¦æ€§é‡‡æ ·æ¼”ç¤º
    print("\n" + "="*60)
    print("ğŸ­ ç¬¬å››æ­¥ï¼šé‡è¦æ€§é‡‡æ ·æ–¹æ³•")
    print("="*60)
    
    # åˆ›å»ºè¡Œä¸ºç­–ç•¥ï¼ˆæ›´éšæœºï¼‰
    behavior_policy = np.array([
        [0.5, 0.5],  # å‡åŒ€ç­–ç•¥
        [0.5, 0.5],
        [0.5, 0.5],
    ])
    
    # ç”Ÿæˆè¡Œä¸ºç­–ç•¥çš„æ•°æ®
    def behavior_simulator(num_episodes=300):
        episodes = []
        for _ in range(num_episodes):
            episode = []
            state = np.random.choice(states)
            
            for _ in range(15):
                state_idx = bellman_solver.state_to_idx[state]
                action_idx = np.random.choice(len(actions), p=behavior_policy[state_idx])
                action = actions[action_idx]
                
                next_state_probs = bellman_solver.P[state_idx, action_idx]
                next_state_idx = np.random.choice(len(states), p=next_state_probs)
                next_state = states[next_state_idx]
                
                reward = bellman_solver.R[state_idx, action_idx, next_state_idx]
                episode.append((state, action, reward))
                state = next_state
            
            episodes.append(episode)
        return episodes
    
    behavior_episodes = behavior_simulator()
    
    V_ordinary_is = mc_methods.importance_sampling_prediction(
        behavior_episodes, behavior_policy, policy, method='ordinary')
    V_weighted_is = mc_methods.importance_sampling_prediction(
        behavior_episodes, behavior_policy, policy, method='weighted')
    
    # 5. å¯è§†åŒ–åˆ†æ
    print("\n" + "="*60)
    print("ğŸ“ˆ ç¬¬äº”æ­¥ï¼šç”Ÿæˆå¯è§†åŒ–åˆ†æ")
    print("="*60)
    
    visualizer = BellmanMonteCarloVisualizer(bellman_solver, mc_methods)
    
    print("1. è´å°”æ›¼æœŸæœ›æ–¹ç¨‹åˆ†è§£å›¾")
    visualizer.plot_bellman_equation_breakdown(policy)
    
    print("2. è’™ç‰¹å¡æ´›æ–¹æ³•æ”¶æ•›åˆ†æ")
    visualizer.plot_mc_convergence_analysis(V_analytical, V_first_visit, V_every_visit)
    
    print("3. é‡è¦æ€§é‡‡æ ·æ–¹æ³•æ¯”è¾ƒ")
    visualizer.plot_importance_sampling_comparison(V_ordinary_is, V_weighted_is, V_analytical)
    
    # 6. æœ€ç»ˆæ€»ç»“
    print("\n" + "="*80)
    print("ğŸ“‹ è´å°”æ›¼æœŸæœ›æ–¹ç¨‹ä¸è’™ç‰¹å¡æ´›æ–¹æ³•åˆ†ææ€»ç»“")
    print("="*80)
    
    print(f"\nğŸ§® è´å°”æ›¼æœŸæœ›æ–¹ç¨‹ç»“æœ:")
    print(f"  è§£æè§£ä¸è¿­ä»£è§£æœ€å¤§å·®å¼‚: {np.max(np.abs(V_analytical - V_iterative)):.8f}")
    
    print(f"\nğŸ² è’™ç‰¹å¡æ´›é¢„æµ‹ç»“æœ:")
    mc_first_error = np.mean(np.abs(V_first_visit - V_analytical))
    mc_every_error = np.mean(np.abs(V_every_visit - V_analytical))
    print(f"  First-Visit MC å¹³å‡è¯¯å·®: {mc_first_error:.4f}")
    print(f"  Every-Visit MC å¹³å‡è¯¯å·®: {mc_every_error:.4f}")
    print(f"  æ›´ä¼˜æ–¹æ³•: {'First-Visit' if mc_first_error < mc_every_error else 'Every-Visit'}")
    
    print(f"\nğŸ­ é‡è¦æ€§é‡‡æ ·ç»“æœ:")
    is_ordinary_error = np.mean(np.abs(V_ordinary_is - V_analytical))
    is_weighted_error = np.mean(np.abs(V_weighted_is - V_analytical))
    print(f"  æ™®é€šé‡è¦æ€§é‡‡æ ·å¹³å‡è¯¯å·®: {is_ordinary_error:.4f}")
    print(f"  åŠ æƒé‡è¦æ€§é‡‡æ ·å¹³å‡è¯¯å·®: {is_weighted_error:.4f}")
    print(f"  æ›´ä¼˜æ–¹æ³•: {'æ™®é€šé‡è¦æ€§é‡‡æ ·' if is_ordinary_error < is_weighted_error else 'åŠ æƒé‡è¦æ€§é‡‡æ ·'}")
    
    print(f"\nğŸ¯ æ ¸å¿ƒæ¦‚å¿µæ€»ç»“:")
    print(f"  1. è´å°”æ›¼æœŸæœ›æ–¹ç¨‹æä¾›äº†ä»·å€¼å‡½æ•°çš„é€’å½’å®šä¹‰")
    print(f"  2. è’™ç‰¹å¡æ´›æ–¹æ³•é€šè¿‡é‡‡æ ·ä¼°è®¡ä»·å€¼å‡½æ•°ï¼Œæ— éœ€æ¨¡å‹")
    print(f"  3. é‡è¦æ€§é‡‡æ ·å…è®¸ä½¿ç”¨ä¸åŒç­–ç•¥çš„æ•°æ®è¿›è¡Œå­¦ä¹ ")
    print(f"  4. åŠ æƒé‡è¦æ€§é‡‡æ ·é€šå¸¸æ¯”æ™®é€šé‡è¦æ€§é‡‡æ ·æ›´ç¨³å®š")


if __name__ == "__main__":
    """ä¸»ç¨‹åºå…¥å£"""
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == '--quick':
        print("ğŸš€ å¿«é€Ÿæ¼”ç¤ºæ¨¡å¼")
        print("="*50)
        
        # å¿«é€Ÿæ¼”ç¤ºæ ¸å¿ƒæ¦‚å¿µ
        states, actions, transition_probs, rewards = create_simple_mdp_example()
        bellman_solver = BellmanExpectationEquation(states, actions, transition_probs, rewards)
        
        # ç®€å•ç­–ç•¥
        policy = np.array([[0.6, 0.4], [0.3, 0.7], [0.1, 0.9]])
        
        # æ±‚è§£è´å°”æ›¼æ–¹ç¨‹
        V = bellman_solver.solve_bellman_expectation_v(policy, method='iterative')
        
        print(f"\nçŠ¶æ€ä»·å€¼å‡½æ•°:")
        for i, state in enumerate(states):
            print(f"  V({state}) = {V[i]:.4f}")
        
        print(f"\nâœ… å¿«é€Ÿæ¼”ç¤ºå®Œæˆï¼è¿è¡Œ 'python bellman_monte_carlo.py' æŸ¥çœ‹å®Œæ•´æ¼”ç¤º")
    
    else:
        # å®Œæ•´æ¼”ç¤º
        demonstrate_bellman_monte_carlo()