"""
é©¬å°”ç§‘å¤«å¥–åŠ±è¿‡ç¨‹ (MRP) è¯¾å ‚æ¼”ç¤ºè„šæœ¬

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„è¯¾å ‚æ¼”ç¤ºè„šæœ¬ï¼Œå±•ç¤ºäº†MRPçš„æ‰€æœ‰æ ¸å¿ƒæ¦‚å¿µã€‚
é€‚åˆåœ¨è¯¾å ‚ä¸Šé€æ­¥æ¼”ç¤ºï¼Œå¸®åŠ©å­¦ç”Ÿç†è§£MRPçš„æ•°å­¦åŽŸç†å’Œå®žé™…åº”ç”¨ã€‚

ä½¿ç”¨æ–¹æ³•:
    python demo.py

ä½œè€…ï¼šæ•™å­¦ç¤ºä¾‹ä»£ç 
ç”¨é€”ï¼šå¼ºåŒ–å­¦ä¹ è¯¾å ‚æ•™å­¦æ¼”ç¤º
"""

import numpy as np
import matplotlib.pyplot as plt
from markov_reward_process import MarkovRewardProcess, MRPVisualizer

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå›¾å½¢æ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False
plt.style.use('seaborn-v0_8')


def classroom_demo():
    """è¯¾å ‚æ¼”ç¤ºä¸»å‡½æ•°"""
    
    print("=" * 80)
    print("é©¬å°”ç§‘å¤«å¥–åŠ±è¿‡ç¨‹ (Markov Reward Process) è¯¾å ‚æ¼”ç¤º")
    print("=" * 80)
    print()
    
    # ========== ç¬¬ä¸€éƒ¨åˆ†ï¼šç†è®ºä»‹ç» ==========
    print("ðŸ“š ç¬¬ä¸€éƒ¨åˆ†ï¼šé©¬å°”ç§‘å¤«å¥–åŠ±è¿‡ç¨‹ç†è®ºåŸºç¡€")
    print("-" * 50)
    print()
    print("é©¬å°”ç§‘å¤«å¥–åŠ±è¿‡ç¨‹ (MRP) æ˜¯ä¸€ä¸ªå››å…ƒç»„ (S, P, R, Î³)ï¼š")
    print("â€¢ S: çŠ¶æ€ç©ºé—´ (State Space)")
    print("â€¢ P: çŠ¶æ€è½¬ç§»æ¦‚çŽ‡çŸ©é˜µ (Transition Probability Matrix)")
    print("â€¢ R: å¥–åŠ±å‡½æ•° (Reward Function)")
    print("â€¢ Î³: æŠ˜æ‰£å› å­ (Discount Factor)")
    print()
    print("æ ¸å¿ƒæ¦‚å¿µï¼š")
    print("â€¢ é©¬å°”ç§‘å¤«æ€§è´¨ï¼šæœªæ¥åªä¾èµ–äºŽå½“å‰çŠ¶æ€ï¼Œä¸ŽåŽ†å²æ— å…³")
    print("â€¢ ä»·å€¼å‡½æ•°ï¼šV(s) = E[G_t | S_t = s]ï¼Œå…¶ä¸­ G_t æ˜¯æŠ˜æ‰£å›žæŠ¥")
    print("â€¢ è´å°”æ›¼æ–¹ç¨‹ï¼šV(s) = R(s) + Î³ * Î£ P(s'|s) * V(s')")
    print()
    
    input("æŒ‰å›žè½¦é”®ç»§ç»­åˆ°å®žé™…ç¤ºä¾‹...")
    
    # ========== ç¬¬äºŒéƒ¨åˆ†ï¼šç®€å•ç¤ºä¾‹ ==========
    print("\n" + "=" * 80)
    print("ðŸ“Š ç¬¬äºŒéƒ¨åˆ†ï¼šå­¦ç”Ÿå­¦ä¹ è¿‡ç¨‹ - ç»å…¸MRPç¤ºä¾‹")
    print("=" * 80)
    
    # åˆ›å»ºå­¦ç”ŸMRP
    states = ['ä¸“å¿ƒå­¦ä¹ ', 'åˆ†å¿ƒ', 'ç¡è§‰']
    
    print(f"\nçŠ¶æ€ç©ºé—´ S = {states}")
    print("\nçŠ¶æ€å«ä¹‰ï¼š")
    print("â€¢ ä¸“å¿ƒå­¦ä¹ ï¼šå­¦ç”Ÿå…¨ç¥žè´¯æ³¨åœ°å­¦ä¹ ï¼ŒèŽ·å¾—çŸ¥è¯†æ”¶ç›Š")
    print("â€¢ åˆ†å¿ƒï¼šå­¦ç”Ÿæ³¨æ„åŠ›ä¸é›†ä¸­ï¼Œå­¦ä¹ æ•ˆæžœå·®")
    print("â€¢ ç¡è§‰ï¼šå­¦ç”Ÿä¼‘æ¯ï¼Œæ—¢ä¸å­¦ä¹ ä¹Ÿä¸å¨±ä¹")
    
    # è½¬ç§»æ¦‚çŽ‡çŸ©é˜µ
    P = np.array([
        [0.7, 0.2, 0.1],  # ä»Žä¸“å¿ƒå­¦ä¹ 
        [0.3, 0.4, 0.3],  # ä»Žåˆ†å¿ƒ
        [0.2, 0.1, 0.7]   # ä»Žç¡è§‰
    ])
    
    print(f"\nçŠ¶æ€è½¬ç§»æ¦‚çŽ‡çŸ©é˜µ P:")
    print("     ä¸“å¿ƒ  åˆ†å¿ƒ  ç¡è§‰")
    for i, state in enumerate(states):
        print(f"{state:>4} {P[i, 0]:.1f}  {P[i, 1]:.1f}  {P[i, 2]:.1f}")
    
    # å¥–åŠ±å‡½æ•°
    R = np.array([10.0, -5.0, 0.0])
    gamma = 0.9
    
    print(f"\nå¥–åŠ±å‡½æ•° R = {R}")
    print("â€¢ ä¸“å¿ƒå­¦ä¹ èŽ·å¾— +10 å¥–åŠ±ï¼ˆå­¦åˆ°çŸ¥è¯†ï¼‰")
    print("â€¢ åˆ†å¿ƒèŽ·å¾— -5 å¥–åŠ±ï¼ˆæµªè´¹æ—¶é—´ï¼‰")
    print("â€¢ ç¡è§‰èŽ·å¾— 0 å¥–åŠ±ï¼ˆä¸­æ€§çŠ¶æ€ï¼‰")
    print(f"\næŠ˜æ‰£å› å­ Î³ = {gamma}")
    
    input("\næŒ‰å›žè½¦é”®åˆ›å»ºMRPå¹¶è®¡ç®—ä»·å€¼å‡½æ•°...")
    
    # åˆ›å»ºMRP
    student_mrp = MarkovRewardProcess(states, P, R, gamma)
    
    # ========== ç¬¬ä¸‰éƒ¨åˆ†ï¼šä»·å€¼å‡½æ•°è®¡ç®— ==========
    print("\n" + "=" * 80)
    print("ðŸ§® ç¬¬ä¸‰éƒ¨åˆ†ï¼šä»·å€¼å‡½æ•°è®¡ç®—")
    print("=" * 80)
    
    print("\næ–¹æ³•1ï¼šè§£æžè§£ (Analytical Solution)")
    print("ä½¿ç”¨è´å°”æ›¼æ–¹ç¨‹çš„çŸ©é˜µå½¢å¼ï¼šV = (I - Î³P)^(-1) * R")
    
    V_analytical = student_mrp.compute_value_function_analytical()
    
    print("\nè§£æžè§£ç»“æžœï¼š")
    for i, state in enumerate(states):
        print(f"V({state}) = {V_analytical[i]:.3f}")
    
    input("\næŒ‰å›žè½¦é”®æ¼”ç¤ºè¿­ä»£è§£...")
    
    print("\næ–¹æ³•2ï¼šä»·å€¼è¿­ä»£ (Value Iteration)")
    print("è¿­ä»£å…¬å¼ï¼šV_{k+1}(s) = R(s) + Î³ * Î£ P(s'|s) * V_k(s')")
    
    V_iterative = student_mrp.compute_value_function_iterative(max_iterations=50)
    
    print(f"\nä»·å€¼è¿­ä»£åœ¨ç¬¬ {len(student_mrp.convergence_history)} æ¬¡è¿­ä»£åŽæ”¶æ•›")
    print("\nè¿­ä»£è§£ç»“æžœï¼š")
    for i, state in enumerate(states):
        print(f"V({state}) = {V_iterative[i]:.3f}")
    
    print("\nè§£æžè§£ä¸Žè¿­ä»£è§£çš„å·®å¼‚ï¼š")
    for i, state in enumerate(states):
        diff = abs(V_analytical[i] - V_iterative[i])
        print(f"{state}: {diff:.6f}")
    
    input("\næŒ‰å›žè½¦é”®æŸ¥çœ‹å¯è§†åŒ–ç»“æžœ...")
    
    # ========== ç¬¬å››éƒ¨åˆ†ï¼šå¯è§†åŒ–æ¼”ç¤º ==========
    print("\n" + "=" * 80)
    print("ðŸ“ˆ ç¬¬å››éƒ¨åˆ†ï¼šå¯è§†åŒ–åˆ†æž")
    print("=" * 80)
    
    visualizer = MRPVisualizer(student_mrp)
    
    print("\næ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    print("1. çŠ¶æ€è½¬ç§»å›¾ - æ˜¾ç¤ºçŠ¶æ€é—´çš„è½¬ç§»å…³ç³»")
    visualizer.plot_state_transition_graph()
    
    print("2. è½¬ç§»æ¦‚çŽ‡çŸ©é˜µçƒ­åŠ›å›¾ - ç›´è§‚æ˜¾ç¤ºè½¬ç§»æ¦‚çŽ‡")
    visualizer.plot_transition_matrix_heatmap()
    
    print("3. ä»·å€¼å‡½æ•°æ¯”è¾ƒ - å¯¹æ¯”ä¸¤ç§è®¡ç®—æ–¹æ³•")
    visualizer.plot_value_function_comparison()
    
    print("4. å¥–åŠ±å‡½æ•°å¯è§†åŒ–")
    visualizer.plot_reward_function()
    
    input("\næŒ‰å›žè½¦é”®è¿›è¡Œå›žåˆæ¨¡æ‹Ÿ...")
    
    # ========== ç¬¬äº”éƒ¨åˆ†ï¼šå›žåˆæ¨¡æ‹Ÿ ==========
    print("\n" + "=" * 80)
    print("ðŸŽ® ç¬¬äº”éƒ¨åˆ†ï¼šå›žåˆæ¨¡æ‹Ÿä¸Žåˆ†æž")
    print("=" * 80)
    
    print("\næ¨¡æ‹Ÿå­¦ç”Ÿä»Ž'ä¸“å¿ƒå­¦ä¹ 'çŠ¶æ€å¼€å§‹çš„å­¦ä¹ è¿‡ç¨‹...")
    
    # å•æ¬¡å›žåˆæ¼”ç¤º
    states_seq, rewards_seq = student_mrp.simulate_episode('ä¸“å¿ƒå­¦ä¹ ', max_steps=10)
    
    print(f"\nå•æ¬¡å›žåˆæ¨¡æ‹Ÿç»“æžœï¼ˆ10æ­¥ï¼‰ï¼š")
    print("æ­¥éª¤  çŠ¶æ€      å¥–åŠ±")
    print("-" * 25)
    for t, (state, reward) in enumerate(zip(states_seq[:-1], rewards_seq)):
        print(f"{t:>2}   {state:<8} {reward:>5.1f}")
    print(f"æœ€ç»ˆçŠ¶æ€: {states_seq[-1]}")
    
    # è®¡ç®—æŠ˜æ‰£å›žæŠ¥
    discounted_return = sum(reward * (gamma ** t) for t, reward in enumerate(rewards_seq))
    print(f"\næŠ˜æ‰£å›žæŠ¥ G = {discounted_return:.3f}")
    
    # å¤šå›žåˆç»Ÿè®¡
    print("\nè¿›è¡Œ100æ¬¡å›žåˆæ¨¡æ‹Ÿè¿›è¡Œç»Ÿè®¡åˆ†æž...")
    returns = []
    for _ in range(100):
        _, rewards = student_mrp.simulate_episode('ä¸“å¿ƒå­¦ä¹ ', max_steps=20)
        G = sum(reward * (gamma ** t) for t, reward in enumerate(rewards))
        returns.append(G)
    
    print(f"100æ¬¡æ¨¡æ‹Ÿç»Ÿè®¡ç»“æžœï¼š")
    print(f"â€¢ å¹³å‡å›žæŠ¥: {np.mean(returns):.3f}")
    print(f"â€¢ æ ‡å‡†å·®: {np.std(returns):.3f}")
    print(f"â€¢ ç†è®ºä»·å€¼ V(ä¸“å¿ƒå­¦ä¹ ): {V_analytical[0]:.3f}")
    print(f"â€¢ æ¨¡æ‹Ÿä¸Žç†è®ºçš„å·®å¼‚: {abs(np.mean(returns) - V_analytical[0]):.3f}")
    
    # å®Œæ•´çš„å›žåˆæ¨¡æ‹Ÿå¯è§†åŒ–
    print("\n5. å›žåˆæ¨¡æ‹Ÿå¯è§†åŒ–åˆ†æž")
    returns, lengths = visualizer.plot_episode_simulation('ä¸“å¿ƒå­¦ä¹ ', n_episodes=20, max_steps=15)
    
    input("\næŒ‰å›žè½¦é”®æŸ¥çœ‹è¯¾å ‚æ€»ç»“...")
    
    # ========== ç¬¬å…­éƒ¨åˆ†ï¼šè¯¾å ‚æ€»ç»“ ==========
    print("\n" + "=" * 80)
    print("ðŸ“ ç¬¬å…­éƒ¨åˆ†ï¼šè¯¾å ‚æ€»ç»“")
    print("=" * 80)
    
    print("\nä»Šå¤©æˆ‘ä»¬å­¦ä¹ äº†é©¬å°”ç§‘å¤«å¥–åŠ±è¿‡ç¨‹çš„æ ¸å¿ƒæ¦‚å¿µï¼š")
    print("\n1. ç†è®ºåŸºç¡€ï¼š")
    print("   â€¢ MRPå››å…ƒç»„ï¼š(S, P, R, Î³)")
    print("   â€¢ é©¬å°”ç§‘å¤«æ€§è´¨ï¼šæ— è®°å¿†æ€§")
    print("   â€¢ ä»·å€¼å‡½æ•°ï¼šæœŸæœ›æŠ˜æ‰£å›žæŠ¥")
    
    print("\n2. æ•°å­¦æ–¹æ³•ï¼š")
    print("   â€¢ è´å°”æ›¼æ–¹ç¨‹ï¼šV(s) = R(s) + Î³ * Î£ P(s'|s) * V(s')")
    print("   â€¢ è§£æžè§£ï¼šV = (I - Î³P)^(-1) * R")
    print("   â€¢ è¿­ä»£è§£ï¼šä»·å€¼è¿­ä»£ç®—æ³•")
    
    print("\n3. å®žé™…åº”ç”¨ï¼š")
    print("   â€¢ å­¦ç”Ÿå­¦ä¹ è¿‡ç¨‹å»ºæ¨¡")
    print("   â€¢ çŠ¶æ€è½¬ç§»æ¦‚çŽ‡çš„è®¾å®š")
    print("   â€¢ å¥–åŠ±å‡½æ•°çš„è®¾è®¡")
    
    print("\n4. è®¡ç®—éªŒè¯ï¼š")
    print("   â€¢ ç†è®ºè®¡ç®—ä¸Žæ¨¡æ‹Ÿç»“æžœçš„ä¸€è‡´æ€§")
    print("   â€¢ ä¸åŒç®—æ³•ç»“æžœçš„å¯¹æ¯”")
    print("   â€¢ æ”¶æ•›æ€§åˆ†æž")
    
    print("\n5. å¯è§†åŒ–åˆ†æžï¼š")
    print("   â€¢ çŠ¶æ€è½¬ç§»å›¾")
    print("   â€¢ ä»·å€¼å‡½æ•°å¯è§†åŒ–")
    print("   â€¢ å›žåˆæ¨¡æ‹Ÿç»Ÿè®¡")
    
    print(f"\nå…³é”®æ•°å€¼å›žé¡¾ï¼š")
    print(f"â€¢ V(ä¸“å¿ƒå­¦ä¹ ) = {V_analytical[0]:.3f}")
    print(f"â€¢ V(åˆ†å¿ƒ) = {V_analytical[1]:.3f}")
    print(f"â€¢ V(ç¡è§‰) = {V_analytical[2]:.3f}")
    
    print("\nðŸ’¡ æ€è€ƒé¢˜ï¼š")
    print("1. å¦‚æžœæé«˜æŠ˜æ‰£å› å­Î³ï¼Œä»·å€¼å‡½æ•°ä¼šå¦‚ä½•å˜åŒ–ï¼Ÿ")
    print("2. å¦‚æžœæ”¹å˜è½¬ç§»æ¦‚çŽ‡ï¼Œå“ªä¸ªçŠ¶æ€çš„ä»·å€¼å˜åŒ–æœ€å¤§ï¼Ÿ")
    print("3. åœ¨ä»€ä¹ˆæƒ…å†µä¸‹è§£æžè§£å¯èƒ½ä¸å­˜åœ¨ï¼Ÿ")
    
    print("\n" + "=" * 80)
    print("è¯¾å ‚æ¼”ç¤ºç»“æŸï¼æ„Ÿè°¢å¤§å®¶çš„å‚ä¸Žï¼")
    print("=" * 80)


def quick_demo():
    """å¿«é€Ÿæ¼”ç¤ºç‰ˆæœ¬ï¼Œé€‚åˆæ—¶é—´è¾ƒçŸ­çš„è¯¾å ‚"""
    print("é©¬å°”ç§‘å¤«å¥–åŠ±è¿‡ç¨‹ - å¿«é€Ÿæ¼”ç¤ºç‰ˆ")
    print("=" * 50)
    
    # åˆ›å»ºç®€å•çš„3çŠ¶æ€MRP
    states = ['å¥½', 'ä¸­', 'å·®']
    P = np.array([[0.6, 0.3, 0.1],
                  [0.2, 0.6, 0.2],
                  [0.1, 0.4, 0.5]])
    R = np.array([10, 0, -10])
    gamma = 0.9
    
    mrp = MarkovRewardProcess(states, P, R, gamma)
    mrp.print_summary()
    
    # è®¡ç®—ä»·å€¼å‡½æ•°
    V = mrp.compute_value_function_analytical()
    print(f"\nä»·å€¼å‡½æ•°ï¼š{dict(zip(states, V))}")
    
    # ç®€å•å¯è§†åŒ–
    visualizer = MRPVisualizer(mrp)
    visualizer.plot_state_transition_graph()


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == '--quick':
        quick_demo()
    else:
        classroom_demo()